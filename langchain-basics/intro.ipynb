{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e867e3",
   "metadata": {},
   "source": [
    "# Langchain for GenAI\n",
    "\n",
    "> Playlist: [CampusX](https://www.youtube.com/playlist?list=PLKnIA16_RmvaTbihpo4MtzVm4XOQa0ER0)\n",
    "\n",
    "> Notes: https://web.goodnotes.com/s/zBlfqECThNwAJhid4A8bGe\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e405d6b3",
   "metadata": {},
   "source": [
    "## Testing out the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac4fd8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate, PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from pprint import pprint\n",
    "import random\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ae9354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()  # Load environment variables from a .env file if present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25e122c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.3.27'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langchain.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca2cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abaff17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638d62ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is **New\\u202fDelhi**.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"what is the capital of india\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f6db1cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of India is **New Delhi**.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_flash_model.invoke(\"what is the capital of india\").content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9039a1",
   "metadata": {},
   "source": [
    "### Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39946a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.03572908416390419,\n",
       " 0.014558478258550167,\n",
       " 0.011592254973948002,\n",
       " -0.08969993889331818,\n",
       " -0.009068180806934834,\n",
       " 0.013664662837982178,\n",
       " 0.011340967379510403,\n",
       " -0.005701108369976282,\n",
       " -0.027033332735300064,\n",
       " 3.775993536692113e-05]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model = GoogleGenerativeAIEmbeddings(model=\"gemini-embedding-001\")\n",
    "embeddings = embedding_model.embed_query(text=\"What's our Q1 revenue?\", output_dimensionality=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "442f4ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c83f8c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    model_kwargs={\"device\": \"cpu\"},\n",
    ")\n",
    "embeddings = embedding_model.embed_query(text=\"What's our Q1 revenue?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "469390e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ac8699",
   "metadata": {},
   "source": [
    "## Langchain Prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f770da5",
   "metadata": {},
   "source": [
    "### Basic Prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08a51825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate, load_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6c604238",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a helpful assistant that can generate a short report on the topic: {paper_input}\n",
    "    The report should be in the style of {style_input} and the length should be {length_input}\n",
    "    \"\"\",\n",
    "    input_variables=[\"paper_input\", \"style_input\", \"length_input\"]\n",
    ")\n",
    "\n",
    "template.save(\"./prompt_templates/template.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3dfce506",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = load_prompt(\"./prompt_templates/template.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "34432a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = template.format(\n",
    "        paper_input=\"Attention is all you need\", style_input=\"Beginner-Friendly\", length_input=\"Short (1-2 paragraphs)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1cea0e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    You are a helpful assistant that can generate a short report on the topic: Attention is all you need\n",
      "    The report should be in the style of Beginner-Friendly and the length should be Short (1-2 paragraphs)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print (prompt)\n",
    "\n",
    "### Messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7c294e",
   "metadata": {},
   "source": [
    "### Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d88b3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1ac7f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of France is **Paris**.'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant that can answer questions and help with tasks.\"),\n",
    "    HumanMessage(content=\"What is the capital of France?\"),\n",
    "]\n",
    "\n",
    "result = model.invoke(messages)\n",
    "\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2d852",
   "metadata": {},
   "source": [
    "### Dynammic list of messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1dcdaaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate(messages=[\n",
    "    SystemMessage(content=\"You are a helpful assistant who is an expert in the domain: {domain}\"),\n",
    "    HumanMessage(content=\"Explain the topic in simple terms: {topic}\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "37fc3c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant who is an expert in the domain: {domain}', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the topic in simple terms: {topic}', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "prompt = chat_template.invoke({\"domain\": \"AI\", \"topic\": \"Self Attention\"})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b60a1e",
   "metadata": {},
   "source": [
    "### The above does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "03bbd30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant who is an expert in the domain: AI', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain the topic in simple terms in 3-5 sentences: Self Attention', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate(messages=[\n",
    "    (\"system\", \"You are a helpful assistant who is an expert in the domain: {domain}\"),\n",
    "    (\"human\", \"Explain the topic in simple terms in 3-5 sentences: {topic}\"),\n",
    "])\n",
    "prompt = chat_template.invoke({\"domain\": \"AI\", \"topic\": \"Self Attention\"})\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69b273c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Self‑attention is a way for a model to look at all the words in a sentence at once and decide how much each word should influence every other word.  \n",
      "For each word, the model creates three vectors—query, key, and value—then compares the query of one word with the keys of all words to get a “similarity score.”  \n",
      "These scores are turned into weights (via a softmax) that say how much attention each word should give to the others, and the weighted sum of the value vectors gives the new representation for that word.  \n",
      "Because every word can attend to every other word, the model captures long‑range relationships and context without needing to process the sentence sequentially.  \n",
      "This mechanism is the core of transformer models, enabling them to understand and generate language efficiently.\n"
     ]
    }
   ],
   "source": [
    "result = model.invoke(prompt)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a16a3",
   "metadata": {},
   "source": [
    "### Message Placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe629272",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_messages = [\n",
    "    HumanMessage(content=\"I want to request a refund for my order #12345.\"),\n",
    "    AIMessage(content=\"Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "59e7906d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I want to request a refund for my order #12345.'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_messages[0].content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55493bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant that can answer questions and help with tasks.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I want to request a refund for my order #12345.', additional_kwargs={}, response_metadata={}), AIMessage(content='Your refund request for order #12345 has been initiated. It will be processed in 3-5 business days.', additional_kwargs={}, response_metadata={}), HumanMessage(content='how many day again?', additional_kwargs={}, response_metadata={})])\n"
     ]
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate(messages=[\n",
    "    (\"system\", \"You are a helpful assistant that can answer questions and help with tasks.\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", \"{query}\"),\n",
    "])\n",
    "\n",
    "prompt = chat_template.invoke({\"query\": \"how many day again?\", \"chat_history\": history_messages})\n",
    "\n",
    "pprint(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "186112e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It will take **3–5 business days** to complete the refund.  \n",
      "That means the processing time is counted only on weekdays (Monday‑Friday), excluding public holidays. If you have any more questions, just let me know!\n"
     ]
    }
   ],
   "source": [
    "response = model.invoke(prompt)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3977873",
   "metadata": {},
   "source": [
    "## Structured Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075c1c27",
   "metadata": {},
   "source": [
    "### Typed Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf059d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated, Optional, Literal, List\n",
    "\n",
    "class Person(TypedDict):\n",
    "    name: str\n",
    "    age: int\n",
    "    email: str\n",
    "\n",
    "person = Person(name=\"John\", age=30, email=\"john@example.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c1cb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "person = Person(name=123, age=30, email=\"john@example.com\") # this is wrong according to the type def, but it will not raise an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bdaa1dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'positive',\n",
      " 'summary': 'The phone offers great camera performance, long battery life, and '\n",
      "            'a reasonable price, with a decent display and solid gaming '\n",
      "            'experience, though it heats up after extended play. Overall, a '\n",
      "            'solid value.'}\n"
     ]
    }
   ],
   "source": [
    "# create a dummy review for a mobile phone in plain text\n",
    "review = \"\"\"\n",
    "This is a decent phone! I love the camera and the battery life is amazing. Also, the price is reasonable. The display is ok, but some more details are that the ppi is 400 and the screen to body ratio is 80%.\n",
    "Gaming experience is good, but the phone gets heated up after 1 hour of gaming.\n",
    "Netwok connectivity is good. Overall, it is a good phone for the price.\n",
    "\"\"\"\n",
    "\n",
    "# create a review schema\n",
    "\n",
    "class Review(TypedDict):\n",
    "    summary: str\n",
    "    sentiment: str\n",
    "\n",
    "structured_model = model.with_structured_output(Review)\n",
    "response = structured_model.invoke(review)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea00eb8",
   "metadata": {},
   "source": [
    "`with_structured_output` with a schema results in a system prompt behind the scenes which results in generating the response which follows the provided shema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3866d0a",
   "metadata": {},
   "source": [
    "### Using Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "713c563f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sentiment': 'positive',\n",
      " 'summary': 'The phone offers a solid camera, long battery life, and a '\n",
      "            'reasonable price. The display is decent with a 400\\u202fppi and '\n",
      "            '80% screen‑to‑body ratio. Gaming is enjoyable but the device '\n",
      "            'tends to heat after an hour. Network connectivity is reliable, '\n",
      "            'making it a good overall value.'}\n"
     ]
    }
   ],
   "source": [
    "class Review(TypedDict):\n",
    "    summary: Annotated[str, \"A summary of the review\"]\n",
    "    sentiment: Annotated[str, \"The sentiment of the review - posutive, negative or neutral\"]\n",
    "\n",
    "structured_model = model.with_structured_output(Review)\n",
    "response = structured_model.invoke(review)\n",
    "pprint(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fdb1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a detailed review for iPhone 17 Air\n",
    "iphone_17_air_review = \"\"\"\n",
    "The iPhone 17 Air represents Apple's boldest design departure in years, delivering an incredibly thin profile that feels almost impossibly light in hand. At just 5.5mm thick, this device pushes the boundaries of engineering while maintaining the premium build quality we expect from Apple. The aerospace-grade aluminum frame feels solid despite its minimal thickness, and the new Ceramic Shield front provides excellent protection without adding bulk. The device comes in four stunning colors: Midnight Black, Starlight Silver, Deep Purple, and a new Ocean Blue that shifts subtly in different lighting conditions.\n",
    "\n",
    "Performance-wise, the iPhone 17 Air doesn't compromise despite its slim form factor. The A18 Bionic chip with its 3nm process delivers exceptional speed and efficiency, handling everything from intensive gaming to professional video editing with ease. The 8GB of unified memory ensures smooth multitasking, and the improved Neural Engine makes AI-powered features incredibly responsive. Battery life is surprisingly robust for such a thin device, easily lasting a full day of heavy usage thanks to the more efficient chip and optimized iOS 18 integration. The new MagSafe wireless charging is faster than ever, reaching 25W speeds that rival many wired chargers.\n",
    "\n",
    "The camera system is where the iPhone 17 Air truly shines, featuring a revolutionary new periscope telephoto lens that somehow fits within the ultra-thin chassis. The main 48MP sensor captures stunning photos with incredible detail and dynamic range, while the new computational photography features produce professional-quality results in challenging lighting conditions. Night mode has been significantly improved, and the new Action mode for video recording delivers gimbal-like stabilization. The front-facing camera now supports 4K ProRes recording, making it perfect for content creators who demand the highest quality.\n",
    "\n",
    "However, the pursuit of thinness does come with some trade-offs. The device can get noticeably warm during intensive tasks, and the reduced internal space means no room for a traditional headphone jack or even the Lightning port - it's USB-C only with wireless charging as the primary power source. The speakers, while clear, lack the depth and bass response of thicker iPhone models. Additionally, the ultra-thin design makes the device feel somewhat fragile, and Apple's recommended case adds back much of the thickness that the Air design eliminates. Despite these minor compromises, the iPhone 17 Air succeeds in creating a truly premium, futuristic smartphone experience that feels like a glimpse into the next decade of mobile technology.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "418c20d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cons': ['Device can get noticeably warm during intensive tasks',\n",
      "          'No headphone jack or Lightning port – USB-C only',\n",
      "          'Speakers lack depth and bass response',\n",
      "          'Ultra-thin design feels somewhat fragile',\n",
      "          \"Apple's recommended case adds back much of the thickness that the \"\n",
      "          'Air design eliminates'],\n",
      " 'key_themes': ['Design and Build',\n",
      "                'Performance and Efficiency',\n",
      "                'Camera System',\n",
      "                'Battery Life and Charging',\n",
      "                'Trade-offs and Limitations'],\n",
      " 'pros': ['Incredibly thin profile at 5.5mm',\n",
      "          'Aerospace-grade aluminum frame',\n",
      "          'Ceramic Shield front protection',\n",
      "          'Stunning color options',\n",
      "          'A18 Bionic chip with 3nm process',\n",
      "          '8GB unified memory',\n",
      "          'Improved Neural Engine',\n",
      "          'Robust battery life',\n",
      "          '25W MagSafe wireless charging',\n",
      "          'Revolutionary periscope telephoto lens',\n",
      "          '48MP main sensor',\n",
      "          'Advanced computational photography',\n",
      "          'Improved night mode',\n",
      "          'Action mode video stabilization',\n",
      "          '4K ProRes front camera'],\n",
      " 'sentiment': 'pos',\n",
      " 'summary': 'The iPhone\\u202f17\\u202fAir is Apple’s boldest design leap, a '\n",
      "            '5.5\\u202fmm ultra‑thin phone that feels surprisingly light yet '\n",
      "            'sturdy thanks to aerospace‑grade aluminum and Ceramic Shield. '\n",
      "            'Powered by the A18\\u202fBionic 3\\u202fnm chip and 8\\u202fGB RAM, '\n",
      "            'it delivers gaming‑grade speed, efficient battery life, and '\n",
      "            '25\\u202fW MagSafe charging. Its camera system shines with a '\n",
      "            'periscope telephoto, 48\\u202fMP sensor, and 4K ProRes front '\n",
      "            'camera, while night and action modes excel. Trade‑offs include '\n",
      "            'heat under load, USB‑C only, weaker speakers, and a fragile feel '\n",
      "            'that a case can mitigate. Overall, a premium, futuristic '\n",
      "            'experience.'}\n"
     ]
    }
   ],
   "source": [
    "class Review(TypedDict):\n",
    "    key_themes: Annotated[list[str], \"The key themes of the review as a list of strings\"]\n",
    "    summary: Annotated[str, \"A brief summary of the review - max 100 words\"]\n",
    "    sentiment: Annotated[Literal[\"pos\", \"neg\", \"neu\"], \"The sentiment of the review\"]\n",
    "    pros: Annotated[Optional[list[str]], \"The pros of the review as a list of strings\"]\n",
    "    cons: Annotated[Optional[list[str]], \"The cons of the review as a list of strings\"]\n",
    "\n",
    "structured_model = model.with_structured_output(Review)\n",
    "response = structured_model.invoke(iphone_17_air_review)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7e7317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cons': [],\n",
      " 'key_themes': ['Design & Build',\n",
      "                'Performance & Efficiency',\n",
      "                'Battery & Charging',\n",
      "                'Camera & Photography',\n",
      "                'User Experience'],\n",
      " 'pros': ['Ultra-thin 5.5mm profile with premium aerospace-grade aluminum '\n",
      "          'frame',\n",
      "          'Exceptional performance from A18 Bionic 3nm chip and 8GB RAM',\n",
      "          'Robust battery life and fast 25W MagSafe charging',\n",
      "          'Revolutionary periscope telephoto lens and 48MP main sensor',\n",
      "          'Advanced computational photography and improved night mode',\n",
      "          'Front camera supports 4K ProRes for creators'],\n",
      " 'sentiment': 'pos',\n",
      " 'summary': 'The iPhone\\u202f17\\u202fAir redefines slimness with a 5.5\\u202fmm '\n",
      "            'chassis that feels surprisingly solid, thanks to aerospace‑grade '\n",
      "            'aluminum and Ceramic Shield. Powered by the A18 Bionic 3\\u202fnm '\n",
      "            'chip and 8\\u202fGB RAM, it delivers gaming‑grade speed, efficient '\n",
      "            'multitasking, and a full day of battery life, complemented by '\n",
      "            '25\\u202fW MagSafe charging. Its standout camera system—48\\u202fMP '\n",
      "            'main sensor, periscope telephoto, and 4K ProRes front '\n",
      "            'camera—offers professional‑level photography and video, while '\n",
      "            'night mode and Action mode shine. Overall, a sleek, '\n",
      "            'high‑performance device that excels in every key area.'}\n"
     ]
    }
   ],
   "source": [
    "# create a detailed review for iPhone 17 Air -- without cons\n",
    "iphone_17_air_review_without_cons = \"\"\"\n",
    "The iPhone 17 Air represents Apple's boldest design departure in years, delivering an incredibly thin profile that feels almost impossibly light in hand. At just 5.5mm thick, this device pushes the boundaries of engineering while maintaining the premium build quality we expect from Apple. The aerospace-grade aluminum frame feels solid despite its minimal thickness, and the new Ceramic Shield front provides excellent protection without adding bulk. The device comes in four stunning colors: Midnight Black, Starlight Silver, Deep Purple, and a new Ocean Blue that shifts subtly in different lighting conditions.\n",
    "\n",
    "Performance-wise, the iPhone 17 Air doesn't compromise despite its slim form factor. The A18 Bionic chip with its 3nm process delivers exceptional speed and efficiency, handling everything from intensive gaming to professional video editing with ease. The 8GB of unified memory ensures smooth multitasking, and the improved Neural Engine makes AI-powered features incredibly responsive. Battery life is surprisingly robust for such a thin device, easily lasting a full day of heavy usage thanks to the more efficient chip and optimized iOS 18 integration. The new MagSafe wireless charging is faster than ever, reaching 25W speeds that rival many wired chargers.\n",
    "\n",
    "The camera system is where the iPhone 17 Air truly shines, featuring a revolutionary new periscope telephoto lens that somehow fits within the ultra-thin chassis. The main 48MP sensor captures stunning photos with incredible detail and dynamic range, while the new computational photography features produce professional-quality results in challenging lighting conditions. Night mode has been significantly improved, and the new Action mode for video recording delivers gimbal-like stabilization. The front-facing camera now supports 4K ProRes recording, making it perfect for content creators who demand the highest quality.\n",
    "\"\"\"\n",
    "\n",
    "structured_model = model.with_structured_output(Review)\n",
    "response = structured_model.invoke(iphone_17_air_review_without_cons)\n",
    "pprint(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e5fca8",
   "metadata": {},
   "source": [
    "This works really well -- but there is no data validation - this can be done by Pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e99f65",
   "metadata": {},
   "source": [
    "### Using Pydantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5946bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "549cc825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='mini'\n"
     ]
    }
   ],
   "source": [
    "class Student(BaseModel):\n",
    "    name: str\n",
    "\n",
    "new_student = {'name': 'mini'}\n",
    "student = Student(**new_student)\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "faeffa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='mini' age=None\n"
     ]
    }
   ],
   "source": [
    "class Student(BaseModel):\n",
    "    name: str = \"mini\" # default value\n",
    "    age: Optional[int] = None # optional field\n",
    "\n",
    "student = Student()\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebe96bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='mini' age=29\n"
     ]
    }
   ],
   "source": [
    "# pydantic does type coercion, whenever possible\n",
    "class Student(BaseModel):\n",
    "    name: str = \"mini\"\n",
    "    age: Optional[int] = None\n",
    "\n",
    "student = Student(name=\"mini\", age=\"29\")\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "41bc3fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='mini' email='mini@example.com'\n"
     ]
    }
   ],
   "source": [
    "# email validation using pydantic\n",
    "from pydantic import EmailStr\n",
    "class User(BaseModel):\n",
    "    name: str\n",
    "    email: EmailStr\n",
    "\n",
    "user = User(name=\"mini\", email=\"mini@example.com\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "784a6771",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for User\nemail\n  value is not a valid email address: There must be something after the @-sign. [type=value_error, input_value='mini@', input_type=str]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[98]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m user = \u001b[43mUser\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmini\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memail\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmini@\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(user)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/shaunak-dev/LLM-Engineering/.venv/lib/python3.12/site-packages/pydantic/main.py:253\u001b[39m, in \u001b[36mBaseModel.__init__\u001b[39m\u001b[34m(self, **data)\u001b[39m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[32m    252\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m validated_self = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[32m    255\u001b[39m     warnings.warn(\n\u001b[32m    256\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m    257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    259\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m    260\u001b[39m     )\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for User\nemail\n  value is not a valid email address: There must be something after the @-sign. [type=value_error, input_value='mini@', input_type=str]"
     ]
    }
   ],
   "source": [
    "user = User(name=\"mini\", email=\"mini@\")\n",
    "print(user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac08465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='mini' age=20 email='mini@example.com' cgpa=9.5\n"
     ]
    }
   ],
   "source": [
    "# using Field in Pydantic\n",
    "from pydantic import Field\n",
    "\n",
    "\n",
    "class Student(BaseModel):\n",
    "    name: str = \"mini\"\n",
    "    age: Optional[int] = None\n",
    "    email: EmailStr \n",
    "    cgpa: float = Field(ge=0, le=10, default=8.0, description=\"The CGPA of the student\") # description is like the annotation in TypedDict - helps the llm understand the field\n",
    "\n",
    "student = Student(name=\"mini\", age=20, email=\"mini@example.com\", cgpa=9.5)\n",
    "print(student)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8675e3a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='mini' age=20 email='mini@example.com' cgpa=9.5\n"
     ]
    }
   ],
   "source": [
    "student = Student(name=\"mini\", age=20, email=\"mini@example.com\", cgpa=9.5)\n",
    "print(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05afd185",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'mini', 'age': 20, 'email': 'mini@example.com', 'cgpa': 9.5}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(student)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1262b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Review(TypedDict):\n",
    "    key_themes: Annotated[list[str], \"The key themes of the review as a list of strings\"]\n",
    "    summary: Annotated[str, \"A brief summary of the review - max 100 words\"]\n",
    "    sentiment: Annotated[Literal[\"pos\", \"neg\", \"neu\"], \"The sentiment of the review\"]\n",
    "    pros: Annotated[Optional[list[str]], \"The pros of the review as a list of strings\"]\n",
    "    cons: Annotated[Optional[list[str]], \"The cons of the review as a list of strings\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a32554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cons': [],\n",
      " 'key_themes': ['design',\n",
      "                'performance',\n",
      "                'battery',\n",
      "                'camera',\n",
      "                'MagSafe',\n",
      "                'color options'],\n",
      " 'pros': ['ultra-thin 5.5mm profile',\n",
      "          'aerospace-grade aluminum frame',\n",
      "          'Ceramic Shield front',\n",
      "          'four vibrant colors',\n",
      "          'A18 Bionic 3nm chip',\n",
      "          '8GB unified memory',\n",
      "          'Neural Engine AI',\n",
      "          'robust battery life',\n",
      "          '25W MagSafe charging',\n",
      "          'periscope telephoto lens',\n",
      "          '48MP main sensor',\n",
      "          'advanced computational photography',\n",
      "          'improved night mode',\n",
      "          'Action mode stabilization',\n",
      "          '4K ProRes front camera'],\n",
      " 'sentiment': 'pos',\n",
      " 'summary': 'The iPhone\\u202f17\\u202fAir redefines slimness with a 5.5\\u202fmm '\n",
      "            'chassis that feels almost weightless yet feels solid thanks to '\n",
      "            'aerospace‑grade aluminum and Ceramic Shield. Powered by the A18 '\n",
      "            'Bionic 3\\u202fnm chip and 8\\u202fGB memory, it delivers '\n",
      "            'gaming‑grade speed, AI responsiveness, and a full day of battery '\n",
      "            'life, topped by 25\\u202fW MagSafe charging. The camera lineup '\n",
      "            'shines with a periscope telephoto, 48\\u202fMP main sensor, and 4K '\n",
      "            'ProRes front camera, all wrapped in four striking colors. '\n",
      "            'Overall, a sleek, high‑performance device that sets new standards '\n",
      "            'for thinness and photography.'}\n"
     ]
    }
   ],
   "source": [
    "# create a detailed review for iPhone 17 Air -- without cons\n",
    "iphone_17_air_review_without_cons = \"\"\"\n",
    "The iPhone 17 Air represents Apple's boldest design departure in years, delivering an incredibly thin profile that feels almost impossibly light in hand. At just 5.5mm thick, this device pushes the boundaries of engineering while maintaining the premium build quality we expect from Apple. The aerospace-grade aluminum frame feels solid despite its minimal thickness, and the new Ceramic Shield front provides excellent protection without adding bulk. The device comes in four stunning colors: Midnight Black, Starlight Silver, Deep Purple, and a new Ocean Blue that shifts subtly in different lighting conditions.\n",
    "\n",
    "Performance-wise, the iPhone 17 Air doesn't compromise despite its slim form factor. The A18 Bionic chip with its 3nm process delivers exceptional speed and efficiency, handling everything from intensive gaming to professional video editing with ease. The 8GB of unified memory ensures smooth multitasking, and the improved Neural Engine makes AI-powered features incredibly responsive. Battery life is surprisingly robust for such a thin device, easily lasting a full day of heavy usage thanks to the more efficient chip and optimized iOS 18 integration. The new MagSafe wireless charging is faster than ever, reaching 25W speeds that rival many wired chargers.\n",
    "\n",
    "The camera system is where the iPhone 17 Air truly shines, featuring a revolutionary new periscope telephoto lens that somehow fits within the ultra-thin chassis. The main 48MP sensor captures stunning photos with incredible detail and dynamic range, while the new computational photography features produce professional-quality results in challenging lighting conditions. Night mode has been significantly improved, and the new Action mode for video recording delivers gimbal-like stabilization. The front-facing camera now supports 4K ProRes recording, making it perfect for content creators who demand the highest quality.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Review(BaseModel):\n",
    "    key_themes: List[str] = Field(description=\"The key themes of the review as a list of strings\")\n",
    "    summary: str = Field(description=\"A brief summary of the review - max 100 words\")\n",
    "    sentiment: Literal[\"pos\", \"neg\", \"neu\"] = Field(description=\"The sentiment of the review - posutive, negative or neutral\")\n",
    "    pros: Optional[List[str]] = Field(description=\"The pros of the review as a list of strings\", default=None)\n",
    "    cons: Optional[List[str]] = Field(description=\"The cons of the review as a list of strings\", default=None)\n",
    "\n",
    "structured_model = model.with_structured_output(Review)\n",
    "response = structured_model.invoke(iphone_17_air_review_without_cons)\n",
    "pprint(dict(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "585c4197",
   "metadata": {},
   "source": [
    "### JSON Schema\n",
    "\n",
    "This is useful when u cannot define the structure using python - say u are restricted to a diff langauge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f4ba4041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Student',\n",
       " 'description': 'A student is a person who is studying at a school or university',\n",
       " 'type': 'object',\n",
       " 'properties': {'name': {'type': 'string',\n",
       "   'description': 'The name of the student'},\n",
       "  'age': {'type': 'integer', 'description': 'The age of the student'}},\n",
       " 'required': ['name']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "    \"title\": \"Student\",\n",
    "    \"description\": \"A student is a person who is studying at a school or university\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The name of the student\"\n",
    "        },\n",
    "        \"age\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"The age of the student\"\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"name\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "543ad7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# review schema\n",
    "# schema\n",
    "json_schema = {\n",
    "  \"title\": \"Review\",\n",
    "  \"type\": \"object\",\n",
    "  \"properties\": {\n",
    "    \"key_themes\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the key themes discussed in the review in a list\"\n",
    "    },\n",
    "    \"summary\": {\n",
    "      \"type\": \"string\",\n",
    "      \"description\": \"A brief summary of the review\"\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "      \"type\": \"string\",\n",
    "      \"enum\": [\"pos\", \"neg\"],\n",
    "      \"description\": \"Return sentiment of the review either negative, positive or neutral\"\n",
    "    },\n",
    "    \"pros\": {\n",
    "      \"type\": [\"array\", \"null\"],\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the pros inside a list\"\n",
    "    },\n",
    "    \"cons\": {\n",
    "      \"type\": [\"array\", \"null\"],\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      },\n",
    "      \"description\": \"Write down all the cons inside a list\"\n",
    "    },\n",
    "    \"name\": {\n",
    "      \"type\": [\"string\", \"null\"],\n",
    "      \"description\": \"Write the name of the reviewer\"\n",
    "    }\n",
    "  },\n",
    "  \"required\": [\"key_themes\", \"summary\", \"sentiment\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9a0dd59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cons': None,\n",
      " 'key_themes': ['Design & Build',\n",
      "                'Performance & Efficiency',\n",
      "                'Battery Life',\n",
      "                'MagSafe Charging',\n",
      "                'Camera System',\n",
      "                'Color Options'],\n",
      " 'name': None,\n",
      " 'pros': ['Ultra‑thin 5.5\\u202fmm profile',\n",
      "          'Lightweight feel',\n",
      "          'Aerospace‑grade aluminum frame',\n",
      "          'Ceramic Shield front',\n",
      "          'Four vibrant color options',\n",
      "          'A18 Bionic 3\\u202fnm chip',\n",
      "          '8\\u202fGB unified memory',\n",
      "          'Neural Engine AI features',\n",
      "          'Full‑day battery life',\n",
      "          '25\\u202fW MagSafe wireless charging',\n",
      "          'Periscope telephoto lens',\n",
      "          '48\\u202fMP main sensor',\n",
      "          'Advanced computational photography',\n",
      "          'Improved Night mode',\n",
      "          'Action mode video stabilization',\n",
      "          '4K ProRes front camera'],\n",
      " 'sentiment': 'pos',\n",
      " 'summary': 'The iPhone\\u202f17\\u202fAir delivers a bold, ultra‑thin design '\n",
      "            'with premium build quality, powerful A18 Bionic performance, '\n",
      "            'robust battery life, fast MagSafe charging, and a standout camera '\n",
      "            'system featuring a periscope telephoto lens and 48\\u202fMP '\n",
      "            'sensor, making it a top choice for both everyday users and '\n",
      "            'content creators.'}\n"
     ]
    }
   ],
   "source": [
    "structured_model = model.with_structured_output(json_schema)\n",
    "\n",
    "response = structured_model.invoke(iphone_17_air_review_without_cons)\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b36743",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "If your llm cannot generate structured ops out of the box, we can use Op Parsers -- these are classes in LangChain that help convert raw llm responses into structured ops\n",
    "\n",
    "These can be used both with models which can and cannot provide structured ops"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483a034",
   "metadata": {},
   "source": [
    "### StrOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8d4c9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser, JsonOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f62e924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st prompt -> detailed report\n",
    "template1 = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "# 2nd prompt -> summary\n",
    "template2 = PromptTemplate(\n",
    "    template='Write a 2 line summary on the following text. /n {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "47ca412a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This comprehensive report traces AI’s evolution from early symbolic systems to today’s foundation models, detailing core technologies, real‑world applications, and the economic, ethical, and governance challenges they pose. It underscores AI’s transformative impact across industries while calling for responsible design, transparency, and global cooperation to harness its benefits and mitigate risks.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template1 | model | parser | template2 | model | parser\n",
    "\n",
    "chain.invoke({\"topic\": \"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231725d",
   "metadata": {},
   "source": [
    "### JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4483fe48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Return a JSON object.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "JsonOutputParser().get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fbf5990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'facts': ['Black holes are regions of spacetime where gravity is so strong '\n",
      "           'that nothing, not even light, can escape once it crosses the event '\n",
      "           'horizon.',\n",
      "           'The size of a black hole is defined by its Schwarzschild radius, '\n",
      "           'which is proportional to its mass (approximately 3 kilometers per '\n",
      "           'solar mass).',\n",
      "           'Supermassive black holes, with masses millions to billions of '\n",
      "           'times that of the Sun, reside at the centers of most galaxies, '\n",
      "           'including our Milky Way.',\n",
      "           'Black holes can grow by accreting matter from their surroundings '\n",
      "           'or by merging with other black holes, a process that emits '\n",
      "           'powerful gravitational waves detectable by observatories like LIGO '\n",
      "           'and Virgo.',\n",
      "           'Despite their name, black holes are not perfect vacuum; they can '\n",
      "           'emit Hawking radiation—a theoretical quantum effect that causes '\n",
      "           'them to lose mass and eventually evaporate over astronomically '\n",
      "           'long timescales.']}\n"
     ]
    }
   ],
   "source": [
    "parser = JsonOutputParser()\n",
    "\n",
    "template = PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | model | parser\n",
    "\n",
    "result = chain.invoke({'topic':'black hole'})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e6337d",
   "metadata": {},
   "source": [
    "Here its returning in JSON but we __cannot control the schema__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf99184",
   "metadata": {},
   "source": [
    "### StructuredOutputParser\n",
    "\n",
    "- we can enforce a schema here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fce57c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5528b111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"fact_1\": string  // The first fact about the topic\\n\\t\"fact_2\": string  // The second fact about the topic\\n\\t\"fact_3\": string  // The third fact about the topic\\n\\t\"fact_4\": string  // The fourth fact about the topic\\n\\t\"fact_5\": string  // The fifth fact about the topic\\n}\\n```'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema = [ResponseSchema(name=\"fact_1\", description=\"The first fact about the topic\"),\n",
    "ResponseSchema(name=\"fact_2\", description=\"The second fact about the topic\"),\n",
    "ResponseSchema(name=\"fact_3\", description=\"The third fact about the topic\"),\n",
    "ResponseSchema(name=\"fact_4\", description=\"The fourth fact about the topic\"),\n",
    "ResponseSchema(name=\"fact_5\", description=\"The fifth fact about the topic\")\n",
    "]\n",
    "\n",
    "parser = StructuredOutputParser.from_response_schemas(schema)\n",
    "\n",
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db863ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fact_1': 'A black hole is a region of spacetime where gravity is so strong that nothing, not even light, can escape from it.',\n",
       " 'fact_2': 'The boundary around a black hole beyond which no escape is possible is called the event horizon.',\n",
       " 'fact_3': 'Most black holes form from the remnants of large stars that collapse in on themselves at the end of their life cycle, known as stellar black holes.',\n",
       " 'fact_4': 'Supermassive black holes, millions to billions of times the mass of our Sun, are found at the center of most large galaxies, including our own Milky Way (Sagittarius A*).',\n",
       " 'fact_5': \"Despite their immense gravity, black holes do not 'suck' things in from vast distances; objects must get very close to be pulled in, and if our Sun were replaced by a black hole of the same mass, Earth would continue to orbit it normally.\"}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Give me 5 facts about {topic} \\n {format_instruction}',\n",
    "    input_variables=['topic'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = template | gemini_flash_model | parser # note: this fails with the openai oss model\n",
    "\n",
    "chain.invoke({'topic':'black hole'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707551d7",
   "metadata": {},
   "source": [
    "Disadv\n",
    "\n",
    "- CANNOT do data validation \n",
    "- even if llm sends wrong format, (eg: str instead of int) we cannot validate that"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c58aa50",
   "metadata": {},
   "source": [
    "### PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "76ab6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# define schema\n",
    "class Person(BaseModel):\n",
    "    name: str = Field(description=\"The name of the person\")\n",
    "    age: int = Field(description=\"The age of the person\", gt=18, lt=150),\n",
    "    city: str = Field(description=\"The city of the person\")\n",
    "\n",
    "# create parser\n",
    "parser = PydanticOutputParser(pydantic_object=Person)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b934d32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaunak.sen/Documents/shaunak-dev/LLM-Engineering/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='The age of the person', metadata=[Gt(gt=18), Lt(lt=150)]),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"The city of the person\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"city\"]}\\n```'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.get_format_instructions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685c2f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text='Give me the name, age and city of the person with details: John is 25 years old and lives in New York \\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"name\": {\"description\": \"The name of the person\", \"title\": \"Name\", \"type\": \"string\"}, \"age\": {\"title\": \"Age\", \"type\": \"integer\"}, \"city\": {\"description\": \"The city of the person\", \"title\": \"City\", \"type\": \"string\"}}, \"required\": [\"name\", \"city\"]}\\n```'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaunak.sen/Documents/shaunak-dev/LLM-Engineering/.venv/lib/python3.12/site-packages/pydantic/json_schema.py:2324: PydanticJsonSchemaWarning: Default value (FieldInfo(annotation=NoneType, required=True, description='The age of the person', metadata=[Gt(gt=18), Lt(lt=150)]),) is not JSON serializable; excluding default from JSON schema [non-serializable-default]\n",
      "  warnings.warn(message, PydanticJsonSchemaWarning)\n"
     ]
    }
   ],
   "source": [
    "template = PromptTemplate(\n",
    "    template='Give me the name, age and city of the person with details: {details} \\n {format_instruction}',\n",
    "    input_variables=['details'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "print (template.invoke({'details':'John is 25 years old and lives in New York'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "156da830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Person(name='John', age=25, city='New York')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = template | model | parser\n",
    "\n",
    "chain.invoke({'details':'John is 25 years old and lives in New York'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1826cbb6",
   "metadata": {},
   "source": [
    "## Chains in LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aa77f6",
   "metadata": {},
   "source": [
    "### Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "23f49e0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Five Fascinating Facts About Black Holes**\n",
      "\n",
      "1. **They’re Not “Vacuum Cleaners”**  \n",
      "   Despite the popular image of a black hole sucking in everything nearby, space around a black hole is essentially empty. Objects only fall in if they cross the event horizon or are on a trajectory that brings them close enough to be captured by the black hole’s gravity.\n",
      "\n",
      "2. **Time Slows Down Near the Event Horizon**  \n",
      "   According to Einstein’s theory of relativity, time runs slower the closer you are to a massive object. Near a black hole’s event horizon, time can dilate to the point where, for an outside observer, an infalling object appears to freeze and fade as it approaches the horizon.\n",
      "\n",
      "3. **They Emit Hawking Radiation**  \n",
      "   In 1974, Stephen Hawking predicted that black holes can emit tiny amounts of thermal radiation due to quantum effects near the event horizon. This “Hawking radiation” means black holes can slowly lose mass and eventually evaporate over astronomically long timescales.\n",
      "\n",
      "4. **The “No‑Hair” Theorem**  \n",
      "   A black hole can be completely described by just three observable properties: mass, electric charge, and angular momentum (spin). All other details about the matter that formed the black hole are lost—hence the phrase “black holes have no hair.”\n",
      "\n",
      "5. **They’re the Ultimate Cosmic Speed Limiters**  \n",
      "   The escape velocity at the event horizon equals the speed of light. Nothing, not even light, can escape once it crosses this boundary. That’s why black holes are invisible—they don’t emit light, but their gravitational influence can be detected through the motion of nearby stars or the bending of light (gravitational lensing).\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template='Generate 5 interesting facts about {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "chain = prompt | model | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({'topic':'black hole'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5123d48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "713e6c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Three‑point summary**\n",
      "\n",
      "1. **Cricket’s structure and reach** – A bat‑and‑ball sport governed by the ICC, played worldwide by over 2.5 billion fans. It exists in five main formats (Test, ODI, T20I, The Hundred, domestic first‑class), each with distinct rules, durations, and audiences.\n",
      "\n",
      "2. **Economic and cultural significance** – Cricket generates a multi‑billion‑dollar global market (e.g., IPL >US$1 billion in 2023). It is a national pastime in South Asia, the Caribbean, Australia, England, and New Zealand, shaping identity, media, and even diplomatic relations.\n",
      "\n",
      "3. **Future trajectory** – Technological innovations (DRS, AI analytics, smart gear), new formats (100‑over matches, hybrid games), and sustainability initiatives are reshaping the sport, while expansion into emerging markets (US, China, Africa) and governance reforms aim to balance commercial growth with tradition.\n"
     ]
    }
   ],
   "source": [
    "prompt_detailed_report = PromptTemplate(\n",
    "    template='Write a detailed report on {topic}',\n",
    "    input_variables=['topic']\n",
    ")\n",
    "\n",
    "prompt_key_facts = PromptTemplate(\n",
    "    template='Give me a 3 point summary of the following text: {text}',\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "chain = prompt_detailed_report | model | StrOutputParser() | prompt_key_facts | model | StrOutputParser()\n",
    "\n",
    "\n",
    "print(chain.invoke({'topic':'cricket'})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "682b6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     +-------------+       \n",
      "     | PromptInput |       \n",
      "     +-------------+       \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "    +----------------+     \n",
      "    | PromptTemplate |     \n",
      "    +----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "      +----------+         \n",
      "      | ChatGroq |         \n",
      "      +----------+         \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "   +-----------------+     \n",
      "   | StrOutputParser |     \n",
      "   +-----------------+     \n",
      "            *              \n",
      "            *              \n",
      "            *              \n",
      "+-----------------------+  \n",
      "| StrOutputParserOutput |  \n",
      "+-----------------------+  \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427d62f4",
   "metadata": {},
   "source": [
    "### Parallel Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "db0433b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    template=\"Generate short and simple notes from the following text: {text}\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "prompt_2 = PromptTemplate(\n",
    "    template=\"Generate 5 short simple QnAs like a quiz from the following text: {text}\",\n",
    "    input_variables=['text']\n",
    ")\n",
    "\n",
    "\n",
    "prompt_3 = PromptTemplate(\n",
    "    template=\"Merge the provided notes and quiz into a single document \\n {notes} \\n {quiz}\",\n",
    "    input_variables=['notes', 'quiz']\n",
    ")\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9a00b0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56b27955",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = model.invoke(\"Generate a detailed report on Attention is all you need paper\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "970a80de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Attention Is All You Need  \n",
      "**Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017).** *Attention Is All You Need*. In *Advances in Neural Information Processing Systems* (NeurIPS 2017).  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. Executive Summary  \n",
      "\n",
      "The 2017 NeurIPS paper “Attention Is All You Need” introduced the **Transformer** architecture, a novel neural network model that dispenses with recurrence and convolution entirely, relying solely on a **self‑attention** mechanism. The Transformer achieved state‑of‑the‑art results on several machine‑translation benchmarks (WMT 2014 English‑German and English‑French) while dramatically reducing training time and enabling far larger parallelism. Its design has since become the backbone of virtually all modern large‑scale language models (BERT, GPT, T5, etc.).\n",
      "\n",
      "Key innovations:\n",
      "\n",
      "| Innovation | What it solves | Impact |\n",
      "|------------|----------------|--------|\n",
      "| **Scaled Dot‑Product Attention** | Efficient, differentiable weighting of input tokens | Core building block |\n",
      "| **Multi‑Head Attention** | Captures multiple representation sub‑spaces | Richer context |\n",
      "| **Positional Encoding (sinusoidal)** | Injects token order without recurrence | Enables parallel training |\n",
      "| **Layer Normalization & Residual Connections** | Stabilizes training | Faster convergence |\n",
      "| **Feed‑Forward Networks (position‑wise)** | Adds non‑linearity | Enhances expressiveness |\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Background & Motivation  \n",
      "\n",
      "### 2.1 Prior Approaches  \n",
      "- **Recurrent Neural Networks (RNNs)** (LSTM/GRU) – sequential processing, limited parallelism, difficulty learning long‑range dependencies.  \n",
      "- **Convolutional Neural Networks (CNNs)** – improved parallelism but still limited receptive field; required many layers for long‑range context.  \n",
      "- **Hybrid RNN‑CNN** – e.g., ConvS2S, still had sequential bottlenecks.\n",
      "\n",
      "### 2.2 Limitations of Recurrence & Convolution  \n",
      "1. **Sequential Bottleneck** – Each token must wait for the previous one.  \n",
      "2. **Gradient Vanishing / Exploding** – Hard to train deep models.  \n",
      "3. **Limited Parallelism** – Training time scales poorly with sequence length.  \n",
      "4. **Difficulty Capturing Global Context** – Convolutions have limited receptive fields; RNNs rely on hidden state.\n",
      "\n",
      "### 2.3 The Idea: “Attention Is All You Need”  \n",
      "- Replace recurrence and convolution with a **self‑attention** mechanism that directly relates every token to every other token in a single layer.  \n",
      "- Use **parallelizable operations** (matrix multiplications) to enable GPU/TPU scaling.  \n",
      "- Introduce **positional encodings** to provide order information without recurrence.\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Model Architecture  \n",
      "\n",
      "The Transformer is a **sequence‑to‑sequence** model composed of an **encoder** and a **decoder**, each a stack of identical layers. The architecture is fully modular and can be adapted to many tasks beyond translation.\n",
      "\n",
      "### 3.1 Notation  \n",
      "\n",
      "- Input sequence: \\(X = (x_1, x_2, \\dots, x_n)\\).  \n",
      "- Embedding dimension: \\(d_{\\text{model}}\\).  \n",
      "- Number of attention heads: \\(h\\).  \n",
      "- Dimension per head: \\(d_k = d_{\\text{model}} / h\\).  \n",
      "- Feed‑forward hidden size: \\(d_{\\text{ff}}\\).  \n",
      "\n",
      "### 3.2 Encoder Layer  \n",
      "\n",
      "```\n",
      "Input:  X (batch, seq_len, d_model)\n",
      "1. Multi‑Head Self‑Attention (MHSA)\n",
      "   Q = XW_Q, K = XW_K, V = XW_V\n",
      "   Attention_i = softmax((Q_i K_i^T) / sqrt(d_k)) V_i\n",
      "   Concatenate heads → X' = Concat(Attention_1,...,Attention_h) W_O\n",
      "2. Add & Norm: X'' = LayerNorm(X + X')\n",
      "3. Position‑wise Feed‑Forward (FFN)\n",
      "   FFN(x) = max(0, xW_1 + b_1)W_2 + b_2\n",
      "4. Add & Norm: Output = LayerNorm(X'' + FFN(X''))\n",
      "```\n",
      "\n",
      "- **Residual connections** (add) + **Layer Normalization** after each sub‑layer.  \n",
      "- **Dropout** applied after attention and FFN.\n",
      "\n",
      "### 3.3 Decoder Layer  \n",
      "\n",
      "```\n",
      "Input:  Y (target sequence), Encoder output E\n",
      "1. Masked Multi‑Head Self‑Attention (causal)\n",
      "   Same as encoder but with mask to prevent attending to future tokens.\n",
      "2. Add & Norm: Y' = LayerNorm(Y + MaskedAttention)\n",
      "3. Multi‑Head Encoder‑Decoder Attention\n",
      "   Q = Y'W_Q, K = EW_K, V = EW_V\n",
      "   Attention = softmax((Q K^T)/sqrt(d_k)) V\n",
      "4. Add & Norm: Y'' = LayerNorm(Y' + Attention)\n",
      "5. Position‑wise FFN\n",
      "6. Add & Norm: Output = LayerNorm(Y'' + FFN(Y''))\n",
      "```\n",
      "\n",
      "- **Causal mask** ensures autoregressive property during training and inference.  \n",
      "- **Encoder‑Decoder attention** allows the decoder to focus on relevant encoder states.\n",
      "\n",
      "### 3.4 Positional Encoding  \n",
      "\n",
      "Two variants:\n",
      "\n",
      "1. **Sinusoidal** (used in the paper):\n",
      "\n",
      "\\[\n",
      "\\text{PE}_{(pos, 2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right), \\quad\n",
      "\\text{PE}_{(pos, 2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
      "\\]\n",
      "\n",
      "2. **Learned** (later variants).\n",
      "\n",
      "The positional encodings are added to token embeddings before feeding into the encoder/decoder.\n",
      "\n",
      "### 3.5 Hyperparameters (Base Model)  \n",
      "\n",
      "| Component | Value |\n",
      "|-----------|-------|\n",
      "| \\(d_{\\text{model}}\\) | 512 |\n",
      "| \\(h\\) | 8 |\n",
      "| \\(d_k = d_v\\) | 64 |\n",
      "| \\(d_{\\text{ff}}\\) | 2048 |\n",
      "| Layers (encoder/decoder) | 6 |\n",
      "| Dropout | 0.1 |\n",
      "| Optimizer | Adam (β1=0.9, β2=0.98, ε=10⁻⁹) |\n",
      "| Learning rate schedule | Warmup + inverse‑sqrt decay |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Training Procedure  \n",
      "\n",
      "1. **Data** – WMT 2014 English‑German (≈4.5M sentence pairs) and English‑French (≈36M).  \n",
      "2. **Tokenization** – Byte‑Pair Encoding (BPE) with 32k merge operations.  \n",
      "3. **Loss** – Cross‑entropy over target tokens.  \n",
      "4. **Optimizer** – Adam with warmup steps (4000) and learning rate schedule:\n",
      "\n",
      "\\[\n",
      "lrate = d_{\\text{model}}^{-0.5} \\min\\!\\left(step^{-0.5}, step \\cdot warmup^{-1.5}\\right)\n",
      "\\]\n",
      "\n",
      "5. **Batching** – Fixed number of tokens per batch (~4096).  \n",
      "6. **Regularization** – Dropout, label smoothing (ε=0.1).  \n",
      "7. **Inference** – Beam search (beam size 4–8), length penalty.\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Experimental Results  \n",
      "\n",
      "| Task | Model | BLEU (English‑German) | BLEU (English‑French) | Training Time (GPU hours) |\n",
      "|------|-------|-----------------------|-----------------------|---------------------------|\n",
      "| WMT 2014 | Transformer (Base) | **28.4** | **41.0** | ~4 days on 8 GPUs |\n",
      "| WMT 2014 | ConvS2S (baseline) | 27.3 | 39.8 | ~6 days |\n",
      "| WMT 2014 | LSTM (baseline) | 24.6 | 36.5 | ~10 days |\n",
      "\n",
      "- **Speed**: Transformer converges in ~4 days vs. ~10 days for LSTM.  \n",
      "- **Accuracy**: 4–5 BLEU points improvement over the best RNN/CNN baselines.  \n",
      "- **Scalability**: Training time scales linearly with GPU count; no sequential bottleneck.\n",
      "\n",
      "The paper also reports ablation studies:\n",
      "\n",
      "- Removing positional encodings → BLEU drops by ~3 points.  \n",
      "- Using learned positional encodings → similar performance.  \n",
      "- Reducing number of heads → slight drop; more heads improve performance.  \n",
      "- Removing residual connections → training diverges.\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Analysis of Core Components  \n",
      "\n",
      "### 6.1 Scaled Dot‑Product Attention  \n",
      "\n",
      "\\[\n",
      "\\text{Attention}(Q, K, V) = \\text{softmax}\\!\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
      "\\]\n",
      "\n",
      "- **Scaling** by \\(\\sqrt{d_k}\\) prevents large dot‑products from pushing softmax into regions with very small gradients.  \n",
      "- **Computational Complexity**: \\(O(n^2 d_{\\text{model}})\\) per layer, but highly parallelizable.\n",
      "\n",
      "### 6.2 Multi‑Head Attention  \n",
      "\n",
      "- Splits queries, keys, values into \\(h\\) sub‑spaces.  \n",
      "- Each head learns a different representation (e.g., syntactic vs. semantic).  \n",
      "- Concatenation + linear projection restores dimensionality.\n",
      "\n",
      "### 6.3 Positional Encoding  \n",
      "\n",
      "- **Sinusoidal** functions allow the model to extrapolate to longer sequences (due to periodicity).  \n",
      "- **Learned** variants (later work) can adapt better to specific datasets but risk overfitting.\n",
      "\n",
      "### 6.4 Layer Normalization vs. Batch Normalization  \n",
      "\n",
      "- LayerNorm operates over the feature dimension, independent of batch size, making it suitable for sequence models where batch sizes can be small.\n",
      "\n",
      "### 6.5 Residual Connections  \n",
      "\n",
      "- Facilitate gradient flow, enabling deeper stacks (12‑layer encoder/decoder in later variants).  \n",
      "- Empirically reduce training loss and improve generalization.\n",
      "\n",
      "---\n",
      "\n",
      "## 7. Impact & Legacy  \n",
      "\n",
      "| Year | Model | Key Contribution | Relation to Transformer |\n",
      "|------|-------|------------------|------------------------|\n",
      "| 2018 | BERT (Devlin et al.) | Masked LM + Next Sentence Prediction | Uses Transformer encoder; pre‑training paradigm |\n",
      "| 2018 | GPT (Radford et al.) | Autoregressive language modeling | Uses Transformer decoder; large‑scale pre‑training |\n",
      "| 2019 | Transformer‑XL (Dai et al.) | Recurrence across segments | Extends Transformer with memory |\n",
      "| 2019 | T5 (Raffel et al.) | Text‑to‑text framework | Uses encoder‑decoder Transformer |\n",
      "| 2020 | GPT‑3 (Brown et al.) | 175B parameters | Massive scaling of Transformer decoder |\n",
      "| 2021 | Switch‑Transformer (Fedus et al.) | Mixture‑of‑Experts | Sparse attention within Transformer |\n",
      "| 2023 | PaLM (Chowdhery et al.) | 540B parameters | Large‑scale multi‑modal Transformer |\n",
      "\n",
      "The Transformer has become the **de‑facto standard** for NLP, and its principles have been adapted to vision (ViT), audio, and multimodal tasks.\n",
      "\n",
      "---\n",
      "\n",
      "## 8. Strengths & Weaknesses  \n",
      "\n",
      "| Strength | Explanation |\n",
      "|----------|-------------|\n",
      "| **Parallelism** | Entire sequence processed in one forward pass. |\n",
      "| **Long‑Range Dependencies** | Direct connections between any two tokens. |\n",
      "| **Modularity** | Easy to stack layers, add heads, or change sub‑modules. |\n",
      "| **Scalability** | Works well on GPUs/TPUs; training time grows sub‑linearly with model size. |\n",
      "| **Transferability** | Encoder/decoder can be fine‑tuned for diverse tasks. |\n",
      "\n",
      "| Weakness | Explanation |\n",
      "|----------|-------------|\n",
      "| **Quadratic Complexity** | Attention cost \\(O(n^2)\\) limits sequence length (though sparse attention variants mitigate). |\n",
      "| **Memory Footprint** | Large models require significant GPU memory; often need model parallelism. |\n",
      "| **Positional Bias** | Fixed positional encodings may not capture complex positional relationships; learned encodings help but add parameters. |\n",
      "| **Interpretability** | Attention weights are not always faithful explanations of model decisions. |\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Extensions & Variants  \n",
      "\n",
      "| Variant | What Changed | Why |\n",
      "|---------|--------------|-----|\n",
      "| **Transformer‑XL** | Recurrence across segments + relative positional encodings | Handles longer contexts without quadratic cost. |\n",
      "| **Sparse Transformer** | Sparse attention patterns (e.g., local + global) | Reduces complexity to \\(O(n \\log n)\\). |\n",
      "| **Linformer** | Low‑rank projection of keys/values | Linear complexity in sequence length. |\n",
      "| **Performer** | Random feature maps for kernelized attention | Approximate attention in linear time. |\n",
      "| **Longformer** | Sliding window + global attention | Efficient for very long documents. |\n",
      "| **Switch‑Transformer** | Mixture‑of‑Experts (MoE) | Sparse activation of experts to scale parameters without quadratic cost. |\n",
      "\n",
      "---\n",
      "\n",
      "## 10. Future Directions  \n",
      "\n",
      "1. **Efficient Attention** – Continued research into linear‑time attention mechanisms (e.g., Performer, Reformer).  \n",
      "2. **Dynamic Positional Encoding** – Learnable or adaptive encodings that capture hierarchical structure.  \n",
      "3. **Multimodal Transformers** – Unified models for vision, audio, and text (e.g., CLIP, DALL‑E).  \n",
      "4. **Sparse & Structured Attention** – Combining locality with global context for long‑form generation.  \n",
      "5. **Interpretability & Robustness** – Better understanding of attention patterns, adversarial robustness.  \n",
      "6. **Hardware‑aware Design** – Optimizing for emerging accelerators (TPUs, GPUs, neuromorphic chips).  \n",
      "\n",
      "---\n",
      "\n",
      "## 11. Key Takeaways  \n",
      "\n",
      "- The Transformer demonstrates that **self‑attention alone** can replace recurrence and convolution for sequence modeling.  \n",
      "- Its **parallelizable architecture** and **scalable training** made it feasible to train massive language models.  \n",
      "- The paper’s **design principles** (scaled dot‑product, multi‑head, positional encoding, residual + layer norm) remain foundational in modern NLP.  \n",
      "- Subsequent research has built upon and extended the Transformer in numerous directions, cementing its status as a cornerstone of deep learning.\n",
      "\n",
      "---\n",
      "\n",
      "## 12. References  \n",
      "\n",
      "1. Vaswani, A., Shazeer, N., Parmar, N., et al. (2017). *Attention Is All You Need*. NeurIPS 2017.  \n",
      "2. Devlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding*. NAACL.  \n",
      "3. Radford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). *Improving Language Understanding by Generative Pre‑Training*.  \n",
      "4. Dai, Z., Yang, Z., Yang, Y., et al. (2019). *Transformer‑XL: Attentive Language Models Beyond a Fixed-Length Context*. arXiv:1901.02860.  \n",
      "5. Raffel, C., Shazeer, N., Roberts, A., et al. (2020). *Exploring the Limits of Transfer Learning with a Unified Text‑to‑Text Transformer*. JMLR.  \n",
      "6. Brown, T. B., Mann, B., Ryder, N., et al. (2020). *Language Models are Few‑Shot Learners*. NeurIPS 2020.  \n",
      "7. Fedus, W., Zoph, B., & Le, Q. V. (2021). *Switch Transformers: Scaling to Trillion‑Parameter Models with Simple Sparsity*. arXiv:2101.03961.  \n",
      "8. Chowdhery, N., et al. (2022). *PaLM: Scaling Language Modeling with Pathways*. arXiv:2204.02311.  \n",
      "\n",
      "---  \n",
      "\n",
      "*Prepared by ChatGPT – OpenAI, 2025‑09‑14.*\n"
     ]
    }
   ],
   "source": [
    "print (text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ad96641a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# “Attention Is All You Need” – Short & Simple Notes + Quiz  \n",
      "*(Vaswani et al., 2017 – NeurIPS)*  \n",
      "\n",
      "---\n",
      "\n",
      "## 1. What It Is  \n",
      "The **Transformer** – a neural‑network architecture that relies **only on self‑attention** (no RNNs or CNNs). It became the foundation for BERT, GPT, T5, and many other state‑of‑the‑art language models.\n",
      "\n",
      "---\n",
      "\n",
      "## 2. Why It Matters  \n",
      "\n",
      "| Benefit | Why It Matters |\n",
      "|---------|----------------|\n",
      "| **Faster training** | Fully parallelizable – no sequential bottleneck. |\n",
      "| **Higher accuracy** | Outperformed RNN/CNN baselines on WMT 2014 (BLEU 28.4 EN‑DE, 41.0 EN‑FR). |\n",
      "| **Scalable** | Works on GPUs/TPUs; training time grows linearly with GPU count. |\n",
      "| **Modular** | Easy to stack, adapt, or extend (e.g., BERT, GPT). |\n",
      "\n",
      "---\n",
      "\n",
      "## 3. Core Ideas  \n",
      "\n",
      "| Idea | Purpose | Effect |\n",
      "|------|---------|--------|\n",
      "| **Scaled Dot‑Product Attention** | Efficiently weighs token relationships | Core building block |\n",
      "| **Multi‑Head Attention** | Learns multiple “views” of the data | Richer context |\n",
      "| **Sinusoidal Positional Encoding** | Gives token order without recurrence | Enables parallelism |\n",
      "| **LayerNorm + Residuals** | Stabilizes training | Faster convergence |\n",
      "| **Position‑wise Feed‑Forward** | Adds non‑linearity | More expressive power |\n",
      "\n",
      "---\n",
      "\n",
      "## 4. Architecture Overview  \n",
      "\n",
      "| Component | Structure (per layer) | Key Hyper‑params (Base) |\n",
      "|-----------|-----------------------|------------------------|\n",
      "| **Encoder** | `Self‑Attention → Add+Norm → FFN → Add+Norm` (×6) | `d_model=512`, `h=8` heads (`d_k=64`), `d_ff=2048`, dropout 0.1 |\n",
      "| **Decoder** | `Masked Self‑Attention → Add+Norm → Encoder‑Decoder Attention → Add+Norm → FFN → Add+Norm` (×6) | Same as encoder |\n",
      "| **Optimizer** | Adam (warm‑up + inverse‑sqrt LR schedule) |  |\n",
      "\n",
      "---\n",
      "\n",
      "## 5. Training Highlights  \n",
      "\n",
      "| Item | Detail |\n",
      "|------|--------|\n",
      "| **Data** | WMT 2014 English‑German / French |\n",
      "| **Tokenization** | Byte‑Pair Encoding (32k merges) |\n",
      "| **Loss** | Cross‑entropy + label smoothing |\n",
      "| **Batch** | ~4096 tokens |\n",
      "| **Training time** | ~4 days on 8 GPUs (vs. ~10 days for LSTM) |\n",
      "| **Results** | BLEU 28.4 (EN‑DE), 41.0 (EN‑FR) – 4–5 BLEU points better than RNN/CNN baselines |\n",
      "\n",
      "---\n",
      "\n",
      "## 6. Strengths  \n",
      "\n",
      "* **Parallel** – no sequential bottleneck.  \n",
      "* **Long‑range dependencies** – direct token‑to‑token connections.  \n",
      "* **Modular** – easy to stack, adapt, or extend.  \n",
      "* **Scalable** – works on GPUs/TPUs; training time grows linearly with GPU count.  \n",
      "\n",
      "---\n",
      "\n",
      "## 7. Weaknesses  \n",
      "\n",
      "* **Quadratic cost** in sequence length (`O(n²)` attention).  \n",
      "* **Memory heavy** for very large models.  \n",
      "* **Positional encoding** may not capture complex order patterns.  \n",
      "* **Attention not always interpretable**.  \n",
      "\n",
      "---\n",
      "\n",
      "## 8. Extensions (post‑2017)  \n",
      "\n",
      "| Model | Key Feature |\n",
      "|-------|-------------|\n",
      "| **BERT** | Encoder‑only, masked LM |\n",
      "| **GPT** | Decoder‑only, autoregressive LM |\n",
      "| **Transformer‑XL** | Recurrence across segments |\n",
      "| **Sparse/Linear attention** (Longformer, Performer) | Reduce `O(n²)` cost |\n",
      "| **Mixture‑of‑Experts** (Switch‑Transformer) | Scale parameters sparsely |\n",
      "\n",
      "---\n",
      "\n",
      "## 9. Take‑away  \n",
      "The Transformer proved that *self‑attention alone* can replace recurrence and convolution for sequence modeling, enabling faster, more accurate, and highly scalable language models that dominate modern NLP.\n",
      "\n",
      "---\n",
      "\n",
      "# Quiz – “Attention Is All You Need”\n",
      "\n",
      "| # | Question | Answer |\n",
      "|---|----------|--------|\n",
      "| 1 | What neural‑network architecture did the 2017 NeurIPS paper introduce? | The **Transformer** architecture. |\n",
      "| 2 | Which two key mechanisms replace recurrence and convolution in the Transformer? | **Self‑attention** (scaled dot‑product) and **multi‑head attention**. |\n",
      "| 3 | How does the Transformer give the model a sense of token order? | By adding **positional encodings** (sinusoidal in the original paper). |\n",
      "| 4 | What two components are added after each sub‑layer to stabilize training? | **Residual connections** and **Layer Normalization**. |\n",
      "| 5 | Name one major language model that uses the Transformer as its backbone. | **BERT**, **GPT**, or **T5** (any one of them). |\n",
      "\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# define the parallel chain\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"notes\": prompt_1 | model | StrOutputParser(),\n",
    "    \"quiz\": prompt_2 | model | StrOutputParser(),\n",
    "})\n",
    "\n",
    "# define the sequential chain for merging\n",
    "\n",
    "merge_chain = prompt_3 | model | parser\n",
    "\n",
    "# combine the parallel and sequential chains\n",
    "\n",
    "chain = parallel_chain | merge_chain\n",
    "\n",
    "print(chain.invoke({\"text\":text}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6882de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          +---------------------------+            \n",
      "          | Parallel<notes,quiz>Input |            \n",
      "          +---------------------------+            \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "          +----------------------------+           \n",
      "          | Parallel<notes,quiz>Output |           \n",
      "          +----------------------------+           \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +----------------+                 \n",
      "                | PromptTemplate |                 \n",
      "                +----------------+                 \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                   +----------+                    \n",
      "                   | ChatGroq |                    \n",
      "                   +----------+                    \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "                +-----------------+                \n",
      "                | StrOutputParser |                \n",
      "                +-----------------+                \n",
      "                         *                         \n",
      "                         *                         \n",
      "                         *                         \n",
      "            +-----------------------+              \n",
      "            | StrOutputParserOutput |              \n",
      "            +-----------------------+              \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba9fb46",
   "metadata": {},
   "source": [
    "### Conditional Chain\n",
    "\n",
    "---- see notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "179658f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableBranch, RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9d31baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedback(BaseModel):\n",
    "    sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2567fa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = PydanticOutputParser(pydantic_object=Feedback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a1b96602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentiment='positive'\n"
     ]
    }
   ],
   "source": [
    "prompt_1 = PromptTemplate(\n",
    "    template=\"Classify the sentiment of the following text in either positive or negative: {text} \\n {format_instruction}\",\n",
    "    input_variables=['text'],\n",
    "    partial_variables={'format_instruction': parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "chain = prompt_1 | model | parser\n",
    "\n",
    "print(chain.invoke({\"text\":\"I am happy\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f9d53d",
   "metadata": {},
   "source": [
    "#### Conditional Chain structure\n",
    "\n",
    "```\n",
    "branch_chain = RunnableBranch(\n",
    "    (condition_1, chain_2),\n",
    "    (condition_2, chain_2),\n",
    "    ...\n",
    "    default chain if no condition is met\n",
    ")\n",
    "```\n",
    "\n",
    "Its like an __if elif ... else__ statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f12754ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_2 =  PromptTemplate(\n",
    "    template=\"Write an appropriate follow up response to the positive feedback: {feedback}\",\n",
    "    input_variables=['feedback']\n",
    ")\n",
    "prompt_3 =  PromptTemplate(\n",
    "    template=\"Write an appropriate follow up response to the negative feedback: {feedback}\",\n",
    "    input_variables=['feedback']\n",
    ")\n",
    "\n",
    "\n",
    "branch_chain = RunnableBranch(\n",
    "   (lambda x: x.sentiment == \"positive\", prompt_2 | model | StrOutputParser()),\n",
    "   (lambda x: x.sentiment == \"negative\", prompt_3 | model | StrOutputParser()),\n",
    "   RunnableLambda(lambda x: \"Could not understand the sentiment\") # why use RunnableLambda? It is to convert the lambda function to a Runnable chain\n",
    ")\n",
    "\n",
    "chain = prompt_1 | model | parser | branch_chain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "406e63b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you so much for your kind words! I’m thrilled to hear you had a great experience. If there’s anything else I can help with or if you have any suggestions for improvement, please let me know. Your feedback truly makes a difference!\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"text\":\"The new phone is great\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "25b98bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thank you for sharing your experience. I’m sorry to hear that things didn’t meet your expectations. Your feedback is important to us, and I’d like to understand what went wrong so we can make things right. Could you please provide a bit more detail about the issue you encountered? Once I have that information, I’ll do my best to resolve it promptly. Thank you for giving us the chance to improve.\n"
     ]
    }
   ],
   "source": [
    "print(chain.invoke({\"text\":\"I want to return this product ASAP\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "737942de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    +-------------+      \n",
      "    | PromptInput |      \n",
      "    +-------------+      \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "   +----------------+    \n",
      "   | PromptTemplate |    \n",
      "   +----------------+    \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "      +----------+       \n",
      "      | ChatGroq |       \n",
      "      +----------+       \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "+----------------------+ \n",
      "| PydanticOutputParser | \n",
      "+----------------------+ \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "       +--------+        \n",
      "       | Branch |        \n",
      "       +--------+        \n",
      "            *            \n",
      "            *            \n",
      "            *            \n",
      "    +--------------+     \n",
      "    | BranchOutput |     \n",
      "    +--------------+     \n"
     ]
    }
   ],
   "source": [
    "chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18867c7a",
   "metadata": {},
   "source": [
    "### Runnables in LangChain\n",
    "\n",
    "\n",
    "--- see notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "329f30f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:08:48.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitializing DummyLLM...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:08:48.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mPredicting with prompt: Hello\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'This is a dummy response'}\n"
     ]
    }
   ],
   "source": [
    "# component 1\n",
    "\n",
    "class DummyLLM():\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing DummyLLM...\")\n",
    "\n",
    "    def predict(self, prompt):\n",
    "\n",
    "        logger.info(f\"Predicting with prompt: {prompt}\")\n",
    "\n",
    "        dummy_responses = [\n",
    "            \"This is a dummy response\",\n",
    "            \"This is another dummy response\",\n",
    "            \"This is yet another dummy response with a different format\"\n",
    "        ]\n",
    "\n",
    "        return {\"response\": random.choice(dummy_responses)}\n",
    "    \n",
    "    \n",
    "dummy_llm = DummyLLM()\n",
    "\n",
    "print(dummy_llm.predict(\"Hello\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1cd00b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component 2\n",
    "\n",
    "class DummyPromptTemplate():\n",
    "\n",
    "    def __init__(self, template, input_variables):\n",
    "        logger.info(\"Initializing DummyPromptTemplate...\")\n",
    "        self.template = template\n",
    "        self.input_variables = input_variables\n",
    "\n",
    "    def format(self, input_dict):\n",
    "        return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53b1504f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:08:48.879\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitializing DummyPromptTemplate...\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello John, I am 25 years old\n"
     ]
    }
   ],
   "source": [
    "template = DummyPromptTemplate(template=\"Hello {name}, I am {age} years old\", input_variables=[\"name\", \"age\"])\n",
    "\n",
    "print(template.format({\"name\":\"John\", \"age\":25}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28b3b76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:08:51.198\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitializing DummyPromptTemplate...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:08:51.199\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mPredicting with prompt: Tell me about topic: AI\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'response': 'This is yet another dummy response with a different format'}\n"
     ]
    }
   ],
   "source": [
    "# now we can use these dummy components to create a simple application\n",
    "\n",
    "template = DummyPromptTemplate(template=\"Tell me about topic: {topic}\", input_variables=[\"topic\"])\n",
    "prompt = template.format({\"topic\":\"AI\"})\n",
    "response = dummy_llm.predict(prompt)\n",
    "print(response)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e06fa0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:09:30.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitializing DummyPromptTemplate...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:09:30.897\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitializing DummyLLM...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:09:30.898\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m6\u001b[0m - \u001b[1mInitializing DummyChain...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# now lets see how chains used to work in LangChain\n",
    "\n",
    "class DummyChain():\n",
    "\n",
    "    def __init__(self, llm, prompt):\n",
    "        logger.info(\"Initializing DummyChain...\")\n",
    "        self.llm = llm\n",
    "        self.prompt = prompt\n",
    "\n",
    "    def run(self, input_dict):\n",
    "        prompt = self.prompt.format(input_dict)\n",
    "        response = self.llm.predict(prompt)\n",
    "        return response\n",
    "\n",
    "\n",
    "\n",
    "template = DummyPromptTemplate(template=\"Tell me about topic: {topic}\", input_variables=[\"topic\"])\n",
    "dummy_llm = DummyLLM()\n",
    "\n",
    "chain = DummyChain(dummy_llm, template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "733441e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:09:32.745\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mPredicting with prompt: Tell me about topic: AI\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'This is yet another dummy response with a different format'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run({\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad4753b",
   "metadata": {},
   "source": [
    "Note here the `.run` method in DummyChain can work with any prompt and LLM -- but what if we need mult llm calls? this cannot be configured easily"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c523d5",
   "metadata": {},
   "source": [
    "#### Using standardized components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88053879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# component 1\n",
    "\n",
    "class DummyLLM():\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing DummyLLM...\")\n",
    "\n",
    "    def predict(self, prompt):\n",
    "\n",
    "        logger.info(f\"Predicting with prompt: {prompt}\")\n",
    "\n",
    "        dummy_responses = [\n",
    "            \"This is a dummy response\",\n",
    "            \"This is another dummy response\",\n",
    "            \"This is yet another dummy response with a different format\"\n",
    "        ]\n",
    "\n",
    "        return {\"response\": random.choice(dummy_responses)}\n",
    "\n",
    "\n",
    "class DummyPromptTemplate():\n",
    "\n",
    "    def __init__(self, template, input_variables):\n",
    "        logger.info(\"Initializing DummyPromptTemplate...\")\n",
    "        self.template = template\n",
    "        self.input_variables = input_variables\n",
    "\n",
    "    def format(self, input_dict):\n",
    "        return self.template.format(**input_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4585f34f",
   "metadata": {},
   "source": [
    "Now in order to make these standardized:\n",
    "\n",
    "- we need to convert these to runnables\n",
    "- each runnable should have a common method: `invoke()`\n",
    "\n",
    "We can do this using __Abstraction__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cbadbe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Runnable(ABC):\n",
    "\n",
    "    @abstractmethod\n",
    "    def invoke(self, input):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7b444636",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class DummyLLM without an implementation for abstract method 'invoke'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     10\u001b[39m         dummy_responses = [\n\u001b[32m     11\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThis is a dummy response\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     12\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThis is another dummy response\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThis is yet another dummy response with a different format\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m         ]\n\u001b[32m     16\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m: random.choice(dummy_responses)}\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m dummy_llm = \u001b[43mDummyLLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Can't instantiate abstract class DummyLLM without an implementation for abstract method 'invoke'"
     ]
    }
   ],
   "source": [
    "class DummyLLM(Runnable):\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing DummyLLM...\")\n",
    "\n",
    "    def predict(self, prompt):\n",
    "\n",
    "        logger.info(f\"Predicting with prompt: {prompt}\")\n",
    "\n",
    "        dummy_responses = [\n",
    "            \"This is a dummy response\",\n",
    "            \"This is another dummy response\",\n",
    "            \"This is yet another dummy response with a different format\"\n",
    "        ]\n",
    "\n",
    "        return {\"response\": random.choice(dummy_responses)}\n",
    "\n",
    "dummy_llm = DummyLLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e40ffb7",
   "metadata": {},
   "source": [
    "__So now we are forced to implement the invoke methods for all classes that inherit from `Runnable`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7193e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:14:39.016\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyLLM...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "class DummyLLM(Runnable):\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing DummyLLM...\")\n",
    "\n",
    "    def predict(self, prompt):\n",
    "        # NOTE: this is the old method\n",
    "        logger.warning(\"This method is deprecated. Please use invoke() instead.\")\n",
    "        logger.info(f\"Predicting with prompt: {prompt}\")\n",
    "\n",
    "        dummy_responses = [\n",
    "            \"This is a dummy response\",\n",
    "            \"This is another dummy response\",\n",
    "            \"This is yet another dummy response with a different format\"\n",
    "        ]\n",
    "\n",
    "        return {\"response\": random.choice(dummy_responses)}\n",
    "\n",
    "    def invoke(self, prompt):\n",
    "\n",
    "        logger.info(f\"Predicting with prompt: {prompt}\")\n",
    "\n",
    "        dummy_responses = [\n",
    "            \"This is a dummy response\",\n",
    "            \"This is another dummy response\",\n",
    "            \"This is yet another dummy response with a different format\"\n",
    "        ]\n",
    "\n",
    "        return {\"response\": random.choice(dummy_responses)}\n",
    "\n",
    "dummy_llm = DummyLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a6486fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:14:59.630\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Hello\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'This is yet another dummy response with a different format'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_llm.invoke(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad0a7211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:15:06.438\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m8\u001b[0m - \u001b[33m\u001b[1mThis method is deprecated. Please use invoke() instead.\u001b[0m\n",
      "\u001b[32m2025-09-20 10:15:06.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mpredict\u001b[0m:\u001b[36m9\u001b[0m - \u001b[1mPredicting with prompt: Hello\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'This is yet another dummy response with a different format'}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_llm.predict(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "98db04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyPromptTemplate(Runnable):\n",
    "\n",
    "    def __init__(self, template, input_variables):\n",
    "        logger.info(\"Initializing DummyPromptTemplate...\")\n",
    "        self.template = template\n",
    "        self.input_variables = input_variables\n",
    "\n",
    "    def format(self, input_dict):\n",
    "        # NOTE: this is the old method\n",
    "        logger.warning(\"This method is deprecated. Please use invoke() instead.\")\n",
    "        return self.template.format(**input_dict)\n",
    "    \n",
    "    def invoke(self, input_dict):\n",
    "        return self.template.format(**input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1edcad",
   "metadata": {},
   "source": [
    "Now that we have standardized the components, we can build a chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7ef230f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunnableConnector(Runnable):\n",
    "    \"\"\"\n",
    "    This is a connector that can be used to connect multiple runnables in a chain\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, runnable_list: list[Runnable]) -> None:\n",
    "        self.runnable_list = runnable_list\n",
    "\n",
    "    def invoke(self, input_data):\n",
    "\n",
    "        # for each runnable in the list, invoke it with the input data\n",
    "        for runnable in self.runnable_list:\n",
    "            logger.debug(f\"Invoking {runnable.__class__.__name__} with input: {input_data}\")\n",
    "            input_data = runnable.invoke(input_data)\n",
    "        return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "271e52a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:22:28.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyPromptTemplate...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:22:28.505\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyLLM...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template = DummyPromptTemplate(template=\"Tell me about topic: {topic}\", input_variables=[\"topic\"])\n",
    "dummy_llm = DummyLLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba2cc7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:22:28.701\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyPromptTemplate with input: {'topic': 'AI'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:22:28.702\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyLLM with input: Tell me about topic: AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:22:28.702\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Tell me about topic: AI\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'This is another dummy response'}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = RunnableConnector([template, dummy_llm])\n",
    "\n",
    "chain.invoke({\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ecb3c9",
   "metadata": {},
   "source": [
    "Now lets build a new component and chain these:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42a46df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyStrOutputParser(Runnable):\n",
    "\n",
    "    def __init__(self):\n",
    "        logger.info(\"Initializing DummyStrOutputParser...\")\n",
    "\n",
    "    def invoke(self, input_data):\n",
    "        return input_data[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b9d51864",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:27:27.119\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyStrOutputParser...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:27:27.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyPromptTemplate with input: {'topic': 'AI'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:27:27.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyLLM with input: Tell me about topic: AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:27:27.120\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Tell me about topic: AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:27:27.120\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyStrOutputParser with input: {'response': 'This is yet another dummy response with a different format'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is yet another dummy response with a different format'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = DummyStrOutputParser()\n",
    "chain = RunnableConnector([template, dummy_llm, parser])\n",
    "\n",
    "chain.invoke({\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bf9c8f",
   "metadata": {},
   "source": [
    "> Note how its so easy now to connect different components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce4c2bc",
   "metadata": {},
   "source": [
    "#### Connecting multiple chains together -- result will also be a runnable\n",
    "\n",
    "\n",
    "Chain1: generate joke about topic\n",
    "\n",
    "Chain2: takes joke and creates an explanation\n",
    "\n",
    "\n",
    "Connect these 2 chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1f2b238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:30:09.143\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyPromptTemplate...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:09.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyPromptTemplate...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:09.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyLLM...\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:09.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m4\u001b[0m - \u001b[1mInitializing DummyStrOutputParser...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "template1 = DummyPromptTemplate(template=\"Generate a joke about topic: {topic}\", input_variables=[\"topic\"])\n",
    "template2 = DummyPromptTemplate(template=\"Create an explanation for the joke: {response}\", input_variables=[\"response\"])\n",
    "\n",
    "dummy_llm = DummyLLM()\n",
    "\n",
    "dummy_parser = DummyStrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09638841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:30:20.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyPromptTemplate with input: {'topic': 'AI'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:20.238\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyLLM with input: Generate a joke about topic: AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:20.238\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Generate a joke about topic: AI\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'response': 'This is yet another dummy response with a different format'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain1 = RunnableConnector([template1, dummy_llm])\n",
    "\n",
    "chain1.invoke({\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "da7f7f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:30:52.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyPromptTemplate with input: {'response': 'This is a joke about AI'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:52.746\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyLLM with input: Create an explanation for the joke: This is a joke about AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:52.747\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Create an explanation for the joke: This is a joke about AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:30:52.747\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyStrOutputParser with input: {'response': 'This is another dummy response'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is another dummy response'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain2 = RunnableConnector([template2, dummy_llm, dummy_parser])\n",
    "\n",
    "chain2.invoke({\"response\":\"This is a joke about AI\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e5ed2ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-09-20 10:31:14.258\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking RunnableConnector with input: {'topic': 'AI'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyPromptTemplate with input: {'topic': 'AI'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.259\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyLLM with input: Generate a joke about topic: AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.259\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Generate a joke about topic: AI\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking RunnableConnector with input: {'response': 'This is a dummy response'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyPromptTemplate with input: {'response': 'This is a dummy response'}\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.260\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyLLM with input: Create an explanation for the joke: This is a dummy response\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.260\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m21\u001b[0m - \u001b[1mPredicting with prompt: Create an explanation for the joke: This is a dummy response\u001b[0m\n",
      "\u001b[32m2025-09-20 10:31:14.261\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36minvoke\u001b[0m:\u001b[36m13\u001b[0m - \u001b[34m\u001b[1mInvoking DummyStrOutputParser with input: {'response': 'This is another dummy response'}\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is another dummy response'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "consolidated_chain = RunnableConnector([chain1, chain2])\n",
    "\n",
    "consolidated_chain.invoke({\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb0e224",
   "metadata": {},
   "source": [
    "### Runnables - more details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4633955",
   "metadata": {},
   "source": [
    "#### RunnableSequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7514a430",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnableSequence, RunnableParallel, RunnablePassthrough, RunnableLambda, RunnableBranch\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "acd3563b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the AI go to therapy?  \\n\\nBecause it had too many *neural* issues and kept *overfitting* on its past!'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=\"Tell me a joke about topic: {topic}\", input_variables=[\"topic\"])\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = RunnableSequence(prompt, model, parser)\n",
    "\n",
    "chain.invoke({\"topic\":\"AI\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5366f9e",
   "metadata": {},
   "source": [
    "#### RunnableParallel\n",
    "\n",
    "- allows multiple runnables to execute in parallel\n",
    "- each runnable receives the same input and processes it independently, producing a dict of ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69349d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'linkedin': '🚀 **Unlocking the Power of LangChain** 🚀  \\n'\n",
      "             '\\n'\n",
      "             'LangChain is redefining how we build AI‑powered applications by '\n",
      "             'turning large language models into modular, composable '\n",
      "             'components. Whether you’re prototyping a chatbot, automating '\n",
      "             'data pipelines, or creating custom LLM workflows, LangChain '\n",
      "             'gives you the flexibility to mix and match tools, integrate '\n",
      "             'APIs, and scale with confidence.  \\n'\n",
      "             '\\n'\n",
      "             'Curious how it can accelerate your next project? Let’s connect '\n",
      "             'and explore the possibilities!  \\n'\n",
      "             '\\n'\n",
      "             '#LangChain #AI #MachineLearning #LLM #TechInnovation '\n",
      "             '#DataScience #OpenSource 🚀',\n",
      " 'tweet': 'Just built a chatbot with LangChain! 🚀 The modular approach makes '\n",
      "          'LLM integration a breeze. #LangChain #AI #LLM #OpenAI'}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = PromptTemplate(template=\"Generate a short tweet about topic: {topic}\", input_variables=[\"topic\"])\n",
    "prompt2 = PromptTemplate(template=\"Generate a short linkedin post about topic: {topic}\", input_variables=[\"topic\"])\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"tweet\": RunnableSequence(prompt1, model, parser),\n",
    "    \"linkedin\": RunnableSequence(prompt2, model, parser)\n",
    "})\n",
    "\n",
    "result = parallel_chain.invoke({\"topic\":\"Langchain\"})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0a91e6c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        +-------------------------------+          \n",
      "        | Parallel<tweet,linkedin>Input |          \n",
      "        +-------------------------------+          \n",
      "                ***             ***                \n",
      "              **                   **              \n",
      "            **                       **            \n",
      "+----------------+              +----------------+ \n",
      "| PromptTemplate |              | PromptTemplate | \n",
      "+----------------+              +----------------+ \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "    +----------+                  +----------+     \n",
      "    | ChatGroq |                  | ChatGroq |     \n",
      "    +----------+                  +----------+     \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "          *                             *          \n",
      "+-----------------+            +-----------------+ \n",
      "| StrOutputParser |            | StrOutputParser | \n",
      "+-----------------+            +-----------------+ \n",
      "                ***             ***                \n",
      "                   **         **                   \n",
      "                     **     **                     \n",
      "        +--------------------------------+         \n",
      "        | Parallel<tweet,linkedin>Output |         \n",
      "        +--------------------------------+         \n"
     ]
    }
   ],
   "source": [
    "parallel_chain.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2c7d97",
   "metadata": {},
   "source": [
    "#### RunnablePassthrough\n",
    "\n",
    "- returns the input as output without modifying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7643dd73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'Langchain'}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passthrough = RunnablePassthrough()\n",
    "\n",
    "passthrough.invoke({\"topic\":\"Langchain\"}) # returns the input as output without modifying it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1b174863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': '**Explanation**\\n'\n",
      "                '\\n'\n",
      "                'The joke plays on two meanings of the word *spell* and on '\n",
      "                'Harry Potter’s iconic broomstick.\\n'\n",
      "                '\\n'\n",
      "                '1. **Broom‑stick loaf** – In the wizarding world a '\n",
      "                '*broomstick* is the magical flying stick Harry uses. The '\n",
      "                'bakery’s loaf is shaped like a broomstick, so the joke '\n",
      "                'imagines a literal “broom‑stick” bread.\\n'\n",
      "                '\\n'\n",
      "                '2. **Spell out a good breakfast** – In everyday English '\n",
      "                '*spell* means to write out letters. In the wizarding world it '\n",
      "                'means to cast a magic spell. Harry wants to “spell” (i.e., '\n",
      "                'write or cast) a good breakfast, so he goes to the bakery to '\n",
      "                'get the broom‑stick loaf.\\n'\n",
      "                '\\n'\n",
      "                'Thus the humor comes from the double‑meaning of *spell* and '\n",
      "                'the visual pun of a broom‑shaped loaf.',\n",
      " 'joke': 'Why did Harry Potter go to the bakery?\\n'\n",
      "         '\\n'\n",
      "         'Because he heard they had a *broom*‑stick loaf—he wanted to *spell* '\n",
      "         'out a good breakfast!'}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = PromptTemplate(template=\"Write a joke about topic: {topic}\", input_variables=[\"topic\"])\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_generator_chain = RunnableSequence(prompt1, model, parser)\n",
    "prompt2 = PromptTemplate(template=\"Write a short explanation for the joke: {joke}\", input_variables=[\"joke\"])\n",
    "joke_explanation_chain = RunnableSequence(prompt2, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": RunnablePassthrough(),\n",
    "    \"explanation\": joke_explanation_chain\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_generator_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({\"topic\":\"Harry Potter\"})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6de778b",
   "metadata": {},
   "source": [
    "#### RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "78b26dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count(text):\n",
    "    return len(text.split(\" \"))\n",
    "\n",
    "runnable_word_count = RunnableLambda(word_count)\n",
    "\n",
    "runnable_word_count.invoke(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc451c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'joke': 'Why did the AI bring a ladder to the data center?\\n'\n",
      "         '\\n'\n",
      "         'Because it heard the cloud was *high* and wanted to *scale* its own '\n",
      "         'expectations!',\n",
      " 'num_words': 24}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = PromptTemplate(template=\"Write a joke about topic: {topic}\", input_variables=[\"topic\"])\n",
    "parser = StrOutputParser()\n",
    "\n",
    "joke_generator_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "parallel_chain = RunnableParallel({\n",
    "    \"joke\": RunnablePassthrough(),\n",
    "    \"num_words\": RunnableLambda(lambda x: len(x.split(\" \")))    \n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_generator_chain, parallel_chain)\n",
    "\n",
    "result = final_chain.invoke({\"topic\":\"AI\"})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "256b35ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': '**Explanation**\\n'\n",
      "                '\\n'\n",
      "                'The joke plays on two common cloud‑computing terms that sound '\n",
      "                'like everyday words:\\n'\n",
      "                '\\n'\n",
      "                '1. **“High”** – In the cloud world, “high” often refers to '\n",
      "                '*high‑availability* or *high‑performance* services.  \\n'\n",
      "                '2. **“Scale”** – “Scaling” means adding more compute or '\n",
      "                'storage resources to handle more load.\\n'\n",
      "                '\\n'\n",
      "                'The punchline turns those technical phrases into literal '\n",
      "                'actions: the AI thinks the cloud is a *high* place it can '\n",
      "                'climb to, so it brings a ladder. And because it wants to '\n",
      "                '“scale” its own expectations, it’s literally trying to reach '\n",
      "                'higher. The humor comes from treating abstract IT concepts as '\n",
      "                'if they were physical objects that need a ladder.',\n",
      " 'joke': 'Why did the AI bring a ladder to the data center?\\n'\n",
      "         '\\n'\n",
      "         'Because it heard the cloud was *high* and wanted to *scale* its own '\n",
      "         'expectations!',\n",
      " 'num_words_in_explanation': 102,\n",
      " 'num_words_in_joke': 24}\n"
     ]
    }
   ],
   "source": [
    "prompt1 = PromptTemplate(template=\"Write a joke about topic: {topic}\", input_variables=[\"topic\"])\n",
    "parser = StrOutputParser()\n",
    "joke_generator_chain = RunnableSequence(prompt1, model, parser)\n",
    "\n",
    "prompt2 = PromptTemplate(template=\"Write a short explanation for the joke: {joke}\", input_variables=[\"joke\"])\n",
    "joke_explanation_chain = RunnableSequence(prompt2, model, parser)\n",
    "\n",
    "parallel_chain_1 = RunnableParallel({\n",
    "    \"joke\": RunnablePassthrough(),\n",
    "    \"num_words_in_joke\": RunnableLambda(lambda x: len(x.split(\" \"))),\n",
    "    \"explanation\": joke_explanation_chain\n",
    "})\n",
    "\n",
    "parallel_chain_2 = RunnableParallel({\n",
    "    \"explanation\": RunnableLambda(lambda x: x['explanation']),\n",
    "    \"joke\": RunnableLambda(lambda x: x['joke']),\n",
    "    \"num_words_in_explanation\": RunnableLambda(lambda x: len(x['explanation'].split(\" \"))),\n",
    "    \"num_words_in_joke\": RunnableLambda(lambda x: x['num_words_in_joke']),\n",
    "})\n",
    "\n",
    "final_chain = RunnableSequence(joke_generator_chain, parallel_chain_1, parallel_chain_2)\n",
    "\n",
    "result = final_chain.invoke({\"topic\":\"AI\"})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e068543a",
   "metadata": {},
   "source": [
    "#### RunnableBranch\n",
    "\n",
    "- like if else statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eb9b61c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general suggestion'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = StrOutputParser()\n",
    "\n",
    "email_classifier_prompt = PromptTemplate(template=\"Classify the email into one of the following categories: complaint, refund request, general suggestion. Only return the category name, no other text.\\n\\nEmail: {email}\", input_variables=[\"email\"])\n",
    "\n",
    "email_classifier_chain = RunnableSequence(email_classifier_prompt, model, parser, RunnableLambda(lambda x: x.lower().strip()))\n",
    "\n",
    "email_classifier_chain.invoke({\"email\":\"good product\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f6416ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am sorry, we cannot refund the product'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "branch_chain = RunnableBranch(\n",
    "    (lambda x: \"complaint\" in x, RunnableLambda(lambda x: \"Oh I am sorry, we will try to fix it\")),\n",
    "    (lambda x: \"refund\" in x, RunnableLambda(lambda x: \"I am sorry, we cannot refund the product\")),\n",
    "    (lambda x: \"general\" in x, RunnableLambda(lambda x: \"Thank you for your email\")),\n",
    "    RunnablePassthrough()\n",
    ")\n",
    "\n",
    "final_chain = RunnableSequence(email_classifier_chain, branch_chain)\n",
    "\n",
    "final_chain.invoke({\"email\":\"I am very unhappy with the product and I want a refund\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "80a1ce81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thank you for your email'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_chain.invoke({\"email\":\"great product\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409f6bd1",
   "metadata": {},
   "source": [
    "### Tools in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac984dfb",
   "metadata": {},
   "source": [
    "#### Built in tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "602cf071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = DuckDuckGoSearchRun()\n",
    "results = search_tool.invoke(\"Who will play the asia cup final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a2da29ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1 hour ago · Asia Cup 2025 final date, teams qualified, live timings and '\n",
      " 'streaming Bangladesh and Pakistan are playing out a thrilling final Super 4 '\n",
      " 'tie which will make or break their campaign this year. 1 day ago · Which '\n",
      " 'teams can still qualify for the Asia Cup 2025 final , and how? Al Jazeera '\n",
      " 'breaks down the qualification scenarios for all four teams before the Asia '\n",
      " 'Cup final in Dubai on Sunday. 50 minutes ago — Pakistan defeated Bangladesh '\n",
      " 'by 11 runs in their Super Fours match in Dubai on Thursday to advance to the '\n",
      " 'final of the Asia Cup 2025, where it will meet ... 18 hours ago — The winner '\n",
      " 'of the match between Bangladesh and Pakistan on Thursday will qualify for '\n",
      " \"the final; India's win meant Pakistan's fortunes remained in their hands ... \"\n",
      " '52 minutes ago · India and Pakistan will play each other in the final of the '\n",
      " '2025 Asia Cup in a historic fixture, which will see the two teams go '\n",
      " 'head-to-head in the summit clash of the continental showpiece event for the '\n",
      " 'first time in history. , Cricket News - Times Now')\n"
     ]
    }
   ],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e3986fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duckduckgo_search\n",
      "A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
      "{'query': {'description': 'search query to look up', 'title': 'Query', 'type': 'string'}}\n"
     ]
    }
   ],
   "source": [
    "print (search_tool.name)\n",
    "print (search_tool.description)\n",
    "print (search_tool.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de94d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing command:\n",
      " whoami\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shaunak.sen/Documents/shaunak-dev/LLM-Engineering/.venv/lib/python3.12/site-packages/langchain_community/tools/shell/tool.py:33: UserWarning: The shell tool has no safeguards by default. Use at your own risk.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'shaunak.sen\\n'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import ShellTool\n",
    "\n",
    "shell_tool = ShellTool()\n",
    "\n",
    "shell_tool.invoke(\"whoami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dcc44e",
   "metadata": {},
   "source": [
    "#### Custom Tools - using @tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e5c7e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two integers\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "result = multiply.invoke({\"a\": 2, \"b\": 3})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5ebff81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two integers\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print (multiply.name)\n",
    "print (multiply.description)\n",
    "print (multiply.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b5454d",
   "metadata": {},
   "source": [
    "Below is what the LLM sees as the tool information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9e4950a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'description': 'Multiply two integers',\n",
      " 'properties': {'a': {'title': 'A', 'type': 'integer'},\n",
      "                'b': {'title': 'B', 'type': 'integer'}},\n",
      " 'required': ['a', 'b'],\n",
      " 'title': 'multiply',\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "pprint (multiply.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5468031c",
   "metadata": {},
   "source": [
    "#### Using StructuredTool and Pydantic\n",
    "\n",
    "\n",
    "- the argument types are strictly enforced here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67aa4b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import StructuredTool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# arg schema using pydantic\n",
    "class MultiplyInput(BaseModel):\n",
    "    a: int = Field(description=\"The first number to multiply\")\n",
    "    b: int = Field(description=\"The second number to multiply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0fb4e62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "def multiply_func(a: int, b: int) -> int:\n",
    "    return a * b\n",
    "\n",
    "multiply_tool = StructuredTool.from_function(\n",
    "    func=multiply_func,\n",
    "    name=\"multiply\",\n",
    "    description=\"Multiply two numbers\",\n",
    "    args_schema=MultiplyInput\n",
    ")\n",
    "\n",
    "result = multiply_tool.invoke({\"a\": 2, \"b\": 3})\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8207e4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Multiply two numbers\n",
      "{'a': {'description': 'The first number to multiply', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'The second number to multiply', 'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print (multiply_tool.name)\n",
    "print (multiply_tool.description)\n",
    "print (multiply_tool.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3bfeb0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'a': {'description': 'The first number to multiply',\n",
      "                      'title': 'A',\n",
      "                      'type': 'integer'},\n",
      "                'b': {'description': 'The second number to multiply',\n",
      "                      'title': 'B',\n",
      "                      'type': 'integer'}},\n",
      " 'required': ['a', 'b'],\n",
      " 'title': 'MultiplyInput',\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "pprint (multiply_tool.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dff4536",
   "metadata": {},
   "source": [
    "#### Using BaseTool\n",
    "\n",
    "- most flexible method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18a0d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import BaseTool\n",
    "from typing import Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "70bf13a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arg schema using pydantic\n",
    "class MultiplyInput(BaseModel):\n",
    "    a: int = Field(description=\"The first number to multiply\")\n",
    "    b: int = Field(description=\"The second number to multiply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "83fc88da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "multiply\n",
      "Multiply two numbers\n",
      "{'a': {'description': 'The first number to multiply', 'title': 'A', 'type': 'integer'}, 'b': {'description': 'The second number to multiply', 'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "class MultiplyTool(BaseTool):\n",
    "    name: str = \"multiply\"\n",
    "    description: str = \"Multiply two numbers\"\n",
    "    args_schema: Type[BaseModel] = MultiplyInput\n",
    "\n",
    "    def _run(self, a: int, b: int) -> int:\n",
    "        return a * b\n",
    "\n",
    "    def _arun(self, a: int, b: int) -> int: # you can implement an async version of the run method\n",
    "        pass\n",
    "\n",
    "multiply_tool = MultiplyTool()\n",
    "\n",
    "result = multiply_tool.invoke({\"a\": 2, \"b\": 3})\n",
    "\n",
    "pprint(result)\n",
    "print (multiply_tool.name)\n",
    "print (multiply_tool.description)\n",
    "print (multiply_tool.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "743dd0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'properties': {'a': {'description': 'The first number to multiply',\n",
      "                      'title': 'A',\n",
      "                      'type': 'integer'},\n",
      "                'b': {'description': 'The second number to multiply',\n",
      "                      'title': 'B',\n",
      "                      'type': 'integer'}},\n",
      " 'required': ['a', 'b'],\n",
      " 'title': 'MultiplyInput',\n",
      " 'type': 'object'}\n"
     ]
    }
   ],
   "source": [
    "pprint (multiply_tool.args_schema.model_json_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a706a9d",
   "metadata": {},
   "source": [
    "#### Toolkits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faad037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "# create some related tools\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two integers\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two integers\n",
    "    \"\"\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "58cb4723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[StructuredTool(name='multiply', description='Multiply two integers', args_schema=<class 'langchain_core.utils.pydantic.multiply'>, func=<function multiply at 0x119a34680>),\n",
      " StructuredTool(name='add', description='Add two integers', args_schema=<class 'langchain_core.utils.pydantic.add'>, func=<function add at 0x119a34220>)]\n"
     ]
    }
   ],
   "source": [
    "class MathToolkit:\n",
    "    def get_tools(self):\n",
    "        return [multiply, add]\n",
    "\n",
    "math_toolkit = MathToolkit()\n",
    "\n",
    "tools = math_toolkit.get_tools()\n",
    "\n",
    "pprint(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb2555c",
   "metadata": {},
   "source": [
    "### Tool Calling in Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d40fa4",
   "metadata": {},
   "source": [
    "#### Define the tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b8342044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Multiply two integers\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "    \n",
    "multiply.invoke({\"a\": 2, \"b\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a405e799",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    reasoning_format=\"parsed\",\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22750a3",
   "metadata": {},
   "source": [
    "#### Bind the tool with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5031d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bind the tools to the model\n",
    "model_with_tools = model.bind_tools([multiply])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a75bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hey! I’m ChatGPT, your friendly AI assistant. How can I help you today?', additional_kwargs={'reasoning_content': 'User: \"hey whats your name\". We should respond with name. We can say \"I\\'m ChatGPT\". No tool needed.'}, response_metadata={'token_usage': {'completion_tokens': 55, 'prompt_tokens': 124, 'total_tokens': 179, 'completion_time': 0.055419608, 'prompt_time': 0.006035482, 'queue_time': 0.042837598, 'total_time': 0.06145509}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_80501ff3a1', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--9e33bb61-9878-42b8-a229-841934ccfc08-0', usage_metadata={'input_tokens': 124, 'output_tokens': 55, 'total_tokens': 179})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_tools.invoke(\"hey whats your name\") # here no tool call is necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701957ba",
   "metadata": {},
   "source": [
    "#### Tool Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fcfafe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model_with_tools.invoke(\"can u multiply -34 and 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7795206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': -34, 'b': 32},\n",
       "  'id': 'fc_38568951-f582-44ae-a5f1-3075ff743717',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.tool_calls # list of tool calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4898b03",
   "metadata": {},
   "source": [
    "> Note that the LLM does not run the tool only suggests which tool should be called and with what arguments\n",
    "\n",
    "- Tool execution is handled by Langchain or by us"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce28c7",
   "metadata": {},
   "source": [
    "#### Tool execultion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e854fec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1088"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke(res.tool_calls[0]['args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398f3771",
   "metadata": {},
   "source": [
    "#### ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a2ffc9",
   "metadata": {},
   "source": [
    "If we directly call the tool function with the arguments we get back the result\n",
    "\n",
    "But note what happens when u send the entire tool call dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9932586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='-1088', name='multiply', tool_call_id='fc_38568951-f582-44ae-a5f1-3075ff743717')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiply.invoke(res.tool_calls[0]) # returns a ToolMessage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19d5700",
   "metadata": {},
   "source": [
    "We can use this `ToolMessage` as part of our chat history with the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1a771",
   "metadata": {},
   "source": [
    "#### Using ToolMessage in a complete flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0282a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8410da46",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = []\n",
    "user_query = \"can u multiply -34 and 32 and give the result in a short sentence\"\n",
    "user_query_message = HumanMessage(content=user_query)\n",
    "messages.append(user_query_message)\n",
    "\n",
    "result = model_with_tools.invoke(messages)\n",
    "messages.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6bd8aebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='can u multiply -34 and 32 and give the result in a short sentence', additional_kwargs={}, response_metadata={}),\n",
      " AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the multiply function. Provide a short sentence with result. Use function.', 'tool_calls': [{'id': 'fc_9e76d4b5-5095-4c8b-9258-b8722708d499', 'function': {'arguments': '{\"a\":-34,\"b\":32}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 136, 'total_tokens': 182, 'completion_time': 0.045285786, 'prompt_time': 0.006588939, 'queue_time': 0.042476551, 'total_time': 0.051874725}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1c9aff63-1b2d-4031-9099-41a28189c529-0', tool_calls=[{'name': 'multiply', 'args': {'a': -34, 'b': 32}, 'id': 'fc_9e76d4b5-5095-4c8b-9258-b8722708d499', 'type': 'tool_call'}], usage_metadata={'input_tokens': 136, 'output_tokens': 46, 'total_tokens': 182})]\n"
     ]
    }
   ],
   "source": [
    "pprint(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69c2796c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'multiply',\n",
       "  'args': {'a': -34, 'b': 32},\n",
       "  'id': 'fc_9e76d4b5-5095-4c8b-9258-b8722708d499',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "479ac94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_result = multiply.invoke(result.tool_calls[0])\n",
    "messages.append(tool_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a77df0b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='can u multiply -34 and 32 and give the result in a short sentence', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the multiply function. Provide a short sentence with result. Use function.', 'tool_calls': [{'id': 'fc_9e76d4b5-5095-4c8b-9258-b8722708d499', 'function': {'arguments': '{\"a\":-34,\"b\":32}', 'name': 'multiply'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 46, 'prompt_tokens': 136, 'total_tokens': 182, 'completion_time': 0.045285786, 'prompt_time': 0.006588939, 'queue_time': 0.042476551, 'total_time': 0.051874725}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--1c9aff63-1b2d-4031-9099-41a28189c529-0', tool_calls=[{'name': 'multiply', 'args': {'a': -34, 'b': 32}, 'id': 'fc_9e76d4b5-5095-4c8b-9258-b8722708d499', 'type': 'tool_call'}], usage_metadata={'input_tokens': 136, 'output_tokens': 46, 'total_tokens': 182}),\n",
       " ToolMessage(content='-1088', name='multiply', tool_call_id='fc_9e76d4b5-5095-4c8b-9258-b8722708d499')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7c6614b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The product of –34 and 32 is –1088.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_tools.invoke(messages).content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770bd422",
   "metadata": {},
   "source": [
    "### Currency Conversion Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27b7cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from langchain_core.tools import InjectedToolArg\n",
    "from typing import Annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "24087e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88.7311"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool\n",
    "def get_conversion_factor(base_currency: str, target_currency: str) -> float:\n",
    "    \"\"\"\n",
    "    Get the conversion factor between two a base currency and a target currency\n",
    "    Example:\n",
    "    get_conversion_factor(base_currency=\"USD\", target_currency=\"INR\")\n",
    "    \"\"\"\n",
    "\n",
    "    url = f\"https://v6.exchangerate-api.com/v6/1e59c09b2cc7e83a7e9bece7/pair/{base_currency}/{target_currency}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data['conversion_rate']\n",
    "\n",
    "get_conversion_factor.invoke({\"base_currency\": \"USD\", \"target_currency\": \"INR\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8980de",
   "metadata": {},
   "source": [
    "`InjectedToolArg` - This is to tell the LLM that `conversion_factor` is an argument that the LLM does not need to fill in this argument which calling it as a tool - the developer will inject this value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "09a82b1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Message from currency conversion bot: the amount is 7500.0'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool\n",
    "def convert_and_display(base_currency_value: float, conversion_factor: Annotated[float, InjectedToolArg]) -> int:\n",
    "    \"\"\"\n",
    "    Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\n",
    "    \"\"\"\n",
    "    amount = base_currency_value * conversion_factor\n",
    "    return f\"Message from currency conversion bot: the amount is {amount}\"\n",
    "\n",
    "\n",
    "convert_and_display.invoke({\"base_currency_value\": 100, \"conversion_factor\": 75})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05d6049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_flash_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "816a3368",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04ae51a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools([get_conversion_factor, convert_and_display])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0e1019ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    HumanMessage(content=\"Give me conversion for 1000 USD to INR as a currencybot message\")\n",
    "]\n",
    "\n",
    "first_result = model_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dc48c223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': \"We need to use the function get_conversion_factor to get factor USD to INR. Then use convert_and_display with base_currency_value? The convert_and_display expects base_currency_value: number. But we need to pass the amount? The function likely uses the conversion factor internally. We need to call get_conversion_factor first. Then call convert_and_display. Let's do that.\", 'tool_calls': [{'id': 'fc_bd33d143-87ad-4286-bb64-620d2a199824', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 209, 'total_tokens': 314, 'completion_time': 0.103140022, 'prompt_time': 0.0101167, 'queue_time': 0.04403916, 'total_time': 0.113256722}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--732fc232-5d7b-44ee-b313-2b57ed81ba96-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_bd33d143-87ad-4286-bb64-620d2a199824', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 105, 'total_tokens': 314})"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "606a342d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'get_conversion_factor',\n",
       "  'args': {'base_currency': 'USD', 'target_currency': 'INR'},\n",
       "  'id': 'fc_bd33d143-87ad-4286-bb64-620d2a199824',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "08b53155",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(first_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "163b7371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the first tool\n",
    "first_tool_result = get_conversion_factor.invoke(first_result.tool_calls[0])\n",
    "messages.append(first_tool_result)  # Add tool result to history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07b1cb81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_bd33d143-87ad-4286-bb64-620d2a199824')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "127da5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Give me conversion for 1000 USD to INR as a currencybot message', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': \"We need to use the function get_conversion_factor to get factor USD to INR. Then use convert_and_display with base_currency_value? The convert_and_display expects base_currency_value: number. But we need to pass the amount? The function likely uses the conversion factor internally. We need to call get_conversion_factor first. Then call convert_and_display. Let's do that.\", 'tool_calls': [{'id': 'fc_bd33d143-87ad-4286-bb64-620d2a199824', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 209, 'total_tokens': 314, 'completion_time': 0.103140022, 'prompt_time': 0.0101167, 'queue_time': 0.04403916, 'total_time': 0.113256722}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--732fc232-5d7b-44ee-b313-2b57ed81ba96-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_bd33d143-87ad-4286-bb64-620d2a199824', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 105, 'total_tokens': 314}),\n",
       " ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_bd33d143-87ad-4286-bb64-620d2a199824')]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b7d6d4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call model again - now it will suggest the second tool\n",
    "second_result = model_with_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "02e46eaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to convert 1000 USD to INR. The conversion factor is 88.7311. So 1000 * 88.7311 = 88731.1 INR. We need to display as a custom CurrencyBot message. There\\'s a function convert_and_display that takes base_currency_value: number. Probably expects the amount in base currency? Or maybe expects the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we need to call convert_and_display with base_currency_value: 1000? Or maybe the converted value? The function signature: convert_and_display({base_currency_value: number}). It likely uses the conversion factor internally. We already got the factor. So we call convert_and_display with base_currency_value: 1000.', 'tool_calls': [{'id': 'fc_17e7b91f-b4c6-4762-9d3e-560a3d0e388d', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 249, 'total_tokens': 449, 'completion_time': 0.197235406, 'prompt_time': 0.012017189, 'queue_time': 0.044729171, 'total_time': 0.209252595}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--9faa3b4b-57bb-47d0-b765-ddde8a1edf77-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000}, 'id': 'fc_17e7b91f-b4c6-4762-9d3e-560a3d0e388d', 'type': 'tool_call'}], usage_metadata={'input_tokens': 249, 'output_tokens': 200, 'total_tokens': 449})"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4d3eb6b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'convert_and_display',\n",
       "  'args': {'base_currency_value': 1000},\n",
       "  'id': 'fc_17e7b91f-b4c6-4762-9d3e-560a3d0e388d',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_result.tool_calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee2db3e",
   "metadata": {},
   "source": [
    "Because we had `InjectedToolArg` on `conversion_factor` this arg is not set\n",
    "\n",
    "So we need to do that ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ae4f777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "second_result.tool_calls[0]['args']['conversion_factor'] = first_tool_result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a36415e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(second_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c8fa506e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Give me conversion for 1000 USD to INR as a currencybot message', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': \"We need to use the function get_conversion_factor to get factor USD to INR. Then use convert_and_display with base_currency_value? The convert_and_display expects base_currency_value: number. But we need to pass the amount? The function likely uses the conversion factor internally. We need to call get_conversion_factor first. Then call convert_and_display. Let's do that.\", 'tool_calls': [{'id': 'fc_5a2a2b58-210d-4236-9f84-df94a7315676', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 209, 'total_tokens': 314, 'completion_time': 0.104664413, 'prompt_time': 0.010126352, 'queue_time': 0.042770898, 'total_time': 0.114790765}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f5a28a1c-7460-4330-b1fa-caefbefdf60b-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_5a2a2b58-210d-4236-9f84-df94a7315676', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 105, 'total_tokens': 314}),\n",
       " ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_5a2a2b58-210d-4236-9f84-df94a7315676'),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to convert 1000 USD to INR. The conversion factor is 88.7311. So 1000 * 88.7311 = 88731.1 INR. We need to display as a custom CurrencyBot message. There\\'s a function convert_and_display that takes base_currency_value: number. Probably expects the amount in base currency? Or maybe expects the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we need to call convert_and_display with base_currency_value: 1000? Or maybe the converted value? The function signature: convert_and_display({base_currency_value: number}). It likely uses the conversion factor internally. We already got the factor. So we call convert_and_display with base_currency_value: 1000.', 'tool_calls': [{'id': 'fc_dc4f931f-da63-43cd-a7ea-d5fbcaf61c86', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 249, 'total_tokens': 449, 'completion_time': 0.197103923, 'prompt_time': 0.01211119, 'queue_time': 0.04272501, 'total_time': 0.209215113}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f1eb9efc-c64a-47c0-89a1-a58576ef0fc3-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000, 'conversion_factor': '88.7311'}, 'id': 'fc_dc4f931f-da63-43cd-a7ea-d5fbcaf61c86', 'type': 'tool_call'}], usage_metadata={'input_tokens': 249, 'output_tokens': 200, 'total_tokens': 449})]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3c60b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke the second tool\n",
    "second_tool_result = convert_and_display.invoke(second_result.tool_calls[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fbda10b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', name='convert_and_display', tool_call_id='fc_dc4f931f-da63-43cd-a7ea-d5fbcaf61c86')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "second_tool_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "be2259de",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(second_tool_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff7e1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Give me conversion for 1000 USD to INR as a currencybot message', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': \"We need to use the function get_conversion_factor to get factor USD to INR. Then use convert_and_display with base_currency_value? The convert_and_display expects base_currency_value: number. But we need to pass the amount? The function likely uses the conversion factor internally. We need to call get_conversion_factor first. Then call convert_and_display. Let's do that.\", 'tool_calls': [{'id': 'fc_5a2a2b58-210d-4236-9f84-df94a7315676', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 105, 'prompt_tokens': 209, 'total_tokens': 314, 'completion_time': 0.104664413, 'prompt_time': 0.010126352, 'queue_time': 0.042770898, 'total_time': 0.114790765}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f5a28a1c-7460-4330-b1fa-caefbefdf60b-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_5a2a2b58-210d-4236-9f84-df94a7315676', 'type': 'tool_call'}], usage_metadata={'input_tokens': 209, 'output_tokens': 105, 'total_tokens': 314}),\n",
       " ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_5a2a2b58-210d-4236-9f84-df94a7315676'),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to convert 1000 USD to INR. The conversion factor is 88.7311. So 1000 * 88.7311 = 88731.1 INR. We need to display as a custom CurrencyBot message. There\\'s a function convert_and_display that takes base_currency_value: number. Probably expects the amount in base currency? Or maybe expects the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we need to call convert_and_display with base_currency_value: 1000? Or maybe the converted value? The function signature: convert_and_display({base_currency_value: number}). It likely uses the conversion factor internally. We already got the factor. So we call convert_and_display with base_currency_value: 1000.', 'tool_calls': [{'id': 'fc_dc4f931f-da63-43cd-a7ea-d5fbcaf61c86', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 200, 'prompt_tokens': 249, 'total_tokens': 449, 'completion_time': 0.197103923, 'prompt_time': 0.01211119, 'queue_time': 0.04272501, 'total_time': 0.209215113}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--f1eb9efc-c64a-47c0-89a1-a58576ef0fc3-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000, 'conversion_factor': '88.7311'}, 'id': 'fc_dc4f931f-da63-43cd-a7ea-d5fbcaf61c86', 'type': 'tool_call'}], usage_metadata={'input_tokens': 249, 'output_tokens': 200, 'total_tokens': 449}),\n",
       " ToolMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', name='convert_and_display', tool_call_id='fc_dc4f931f-da63-43cd-a7ea-d5fbcaf61c86')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages # now we have all the required messages in the chat history to answer the user query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "826ab4a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Message from currency conversion bot: the amount is 88731.09999999999'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model_with_tools.invoke(messages)\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29f8282",
   "metadata": {},
   "source": [
    "Lets clean up this process and make it more functional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "3c4d1633",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatGroq(\n",
    "    model=\"openai/gpt-oss-20b\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a7736ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_tools = model.bind_tools([get_conversion_factor, convert_and_display])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ce34554c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-02 13:03:33.096\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='can u convert 1000 USD to INR and display the result as a currencybot message' additional_kwargs={} response_metadata={}\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:33.643\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='' additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0' tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}] usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:33.644\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m21\u001b[0m - \u001b[34m\u001b[1mtool_call: {'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-02 13:03:34.288\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m23\u001b[0m - \u001b[34m\u001b[1mget_conversion_factor_result: content='88.7311' name='get_conversion_factor' tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:34.289\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='88.7311' name='get_conversion_factor' tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:34.472\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='' additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0' tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}] usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324}\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:34.473\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mtool_call: {'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:34.474\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='Message from currency conversion bot: the amount is 7873208.10721' name='convert_and_display' tool_call_id='fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}), ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7')]\n",
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}), ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324})]\n",
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}), ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1, 'conversion_factor': '88.7311'}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324}), ToolMessage(content='Message from currency conversion bot: the amount is 7873208.10721', name='convert_and_display', tool_call_id='fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-02 13:03:34.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='' additional_kwargs={'reasoning_content': 'We need to check the logic. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". We called get_conversion_factor with USD to INR. It returned 88.7311. Then we called convert_and_display with base_currency_value: 88731.1. Wait, base_currency_value should be the amount in base currency? The function convert_and_display expects base_currency_value: number. But the example says: convert_and_display(base_currency_value: number). The function likely expects the base currency value to convert. But we passed 88731.1, which is the result of 1000 * 88.7311. That seems wrong. The function likely expects the base currency value (1000) and will internally multiply by conversion factor. But we passed the converted amount. So the function returned 7873208.10721, which is 88731.1 * 88.7311? Let\\'s check: 88731.1 * 88.7311 = 7873208.10721. Yes. So the function double multiplied. So we need to correct: call convert_and_display with base_currency_value: 1000. And maybe pass conversion_factor? The function signature only has base_currency_value. But maybe the function uses the conversion factor internally. So we should call convert_and_display with base_currency_value: 1000. Let\\'s do that.', 'tool_calls': [{'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 321, 'prompt_tokens': 311, 'total_tokens': 632, 'completion_time': 0.315945389, 'prompt_time': 0.016653005, 'queue_time': 0.043721565, 'total_time': 0.332598394}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None} id='run--5057b32e-7874-4ac3-8762-46e0394b17d2-0' tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000}, 'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'type': 'tool_call'}] usage_metadata={'input_tokens': 311, 'output_tokens': 321, 'total_tokens': 632}\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:34.905\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m27\u001b[0m - \u001b[34m\u001b[1mtool_call: {'name': 'convert_and_display', 'args': {'base_currency_value': 1000}, 'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'type': 'tool_call'}\u001b[0m\n",
      "\u001b[32m2025-10-02 13:03:34.906\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m12\u001b[0m - \u001b[34m\u001b[1mlatest_message: content='Message from currency conversion bot: the amount is 88731.09999999999' name='convert_and_display' tool_call_id='fc_3ea0ff73-1d90-4143-acd5-2a7edca79710'\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}), ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1, 'conversion_factor': '88.7311'}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324}), ToolMessage(content='Message from currency conversion bot: the amount is 7873208.10721', name='convert_and_display', tool_call_id='fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to check the logic. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". We called get_conversion_factor with USD to INR. It returned 88.7311. Then we called convert_and_display with base_currency_value: 88731.1. Wait, base_currency_value should be the amount in base currency? The function convert_and_display expects base_currency_value: number. But the example says: convert_and_display(base_currency_value: number). The function likely expects the base currency value to convert. But we passed 88731.1, which is the result of 1000 * 88.7311. That seems wrong. The function likely expects the base currency value (1000) and will internally multiply by conversion factor. But we passed the converted amount. So the function returned 7873208.10721, which is 88731.1 * 88.7311? Let\\'s check: 88731.1 * 88.7311 = 7873208.10721. Yes. So the function double multiplied. So we need to correct: call convert_and_display with base_currency_value: 1000. And maybe pass conversion_factor? The function signature only has base_currency_value. But maybe the function uses the conversion factor internally. So we should call convert_and_display with base_currency_value: 1000. Let\\'s do that.', 'tool_calls': [{'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 321, 'prompt_tokens': 311, 'total_tokens': 632, 'completion_time': 0.315945389, 'prompt_time': 0.016653005, 'queue_time': 0.043721565, 'total_time': 0.332598394}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5057b32e-7874-4ac3-8762-46e0394b17d2-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000}, 'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 321, 'total_tokens': 632})]\n",
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}), ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1, 'conversion_factor': '88.7311'}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324}), ToolMessage(content='Message from currency conversion bot: the amount is 7873208.10721', name='convert_and_display', tool_call_id='fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to check the logic. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". We called get_conversion_factor with USD to INR. It returned 88.7311. Then we called convert_and_display with base_currency_value: 88731.1. Wait, base_currency_value should be the amount in base currency? The function convert_and_display expects base_currency_value: number. But the example says: convert_and_display(base_currency_value: number). The function likely expects the base currency value to convert. But we passed 88731.1, which is the result of 1000 * 88.7311. That seems wrong. The function likely expects the base currency value (1000) and will internally multiply by conversion factor. But we passed the converted amount. So the function returned 7873208.10721, which is 88731.1 * 88.7311? Let\\'s check: 88731.1 * 88.7311 = 7873208.10721. Yes. So the function double multiplied. So we need to correct: call convert_and_display with base_currency_value: 1000. And maybe pass conversion_factor? The function signature only has base_currency_value. But maybe the function uses the conversion factor internally. So we should call convert_and_display with base_currency_value: 1000. Let\\'s do that.', 'tool_calls': [{'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 321, 'prompt_tokens': 311, 'total_tokens': 632, 'completion_time': 0.315945389, 'prompt_time': 0.016653005, 'queue_time': 0.043721565, 'total_time': 0.332598394}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5057b32e-7874-4ac3-8762-46e0394b17d2-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000, 'conversion_factor': '88.7311'}, 'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 321, 'total_tokens': 632}), ToolMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', name='convert_and_display', tool_call_id='fc_3ea0ff73-1d90-4143-acd5-2a7edca79710')]\n",
      "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}), ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1, 'conversion_factor': '88.7311'}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324}), ToolMessage(content='Message from currency conversion bot: the amount is 7873208.10721', name='convert_and_display', tool_call_id='fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c'), AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to check the logic. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". We called get_conversion_factor with USD to INR. It returned 88.7311. Then we called convert_and_display with base_currency_value: 88731.1. Wait, base_currency_value should be the amount in base currency? The function convert_and_display expects base_currency_value: number. But the example says: convert_and_display(base_currency_value: number). The function likely expects the base currency value to convert. But we passed 88731.1, which is the result of 1000 * 88.7311. That seems wrong. The function likely expects the base currency value (1000) and will internally multiply by conversion factor. But we passed the converted amount. So the function returned 7873208.10721, which is 88731.1 * 88.7311? Let\\'s check: 88731.1 * 88.7311 = 7873208.10721. Yes. So the function double multiplied. So we need to correct: call convert_and_display with base_currency_value: 1000. And maybe pass conversion_factor? The function signature only has base_currency_value. But maybe the function uses the conversion factor internally. So we should call convert_and_display with base_currency_value: 1000. Let\\'s do that.', 'tool_calls': [{'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 321, 'prompt_tokens': 311, 'total_tokens': 632, 'completion_time': 0.315945389, 'prompt_time': 0.016653005, 'queue_time': 0.043721565, 'total_time': 0.332598394}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5057b32e-7874-4ac3-8762-46e0394b17d2-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000, 'conversion_factor': '88.7311'}, 'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 321, 'total_tokens': 632}), ToolMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', name='convert_and_display', tool_call_id='fc_3ea0ff73-1d90-4143-acd5-2a7edca79710'), AIMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', additional_kwargs={'reasoning_content': 'We need to respond to the user. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". The assistant already performed the conversion and displayed the result. The last message from the function is \"Message from currency conversion bot: the amount is 88731.09999999999\". That is the correct conversion. We should just output that message. But we need to ensure we don\\'t call the function again. The last function call returned the message. So we should just output that message. The user might want a final answer. So we should output the message.'}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 369, 'total_tokens': 521, 'completion_time': 0.149464805, 'prompt_time': 0.019251342, 'queue_time': 0.043139948, 'total_time': 0.168716147}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e2f73d5e-27e0-4f9a-b589-458c84c3e5a8-0', usage_metadata={'input_tokens': 369, 'output_tokens': 152, 'total_tokens': 521})]\n"
     ]
    }
   ],
   "source": [
    "user_query = \"can u convert 1000 USD to INR and display the result as a currencybot message\"\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(content=user_query)\n",
    "]\n",
    "\n",
    "latest_message = messages[-1]\n",
    "conversion_factor = None\n",
    "\n",
    "while not(latest_message.type == \"ai\" and len(getattr(latest_message, 'tool_calls', [])) == 0):\n",
    "    \n",
    "    logger.debug(f\"latest_message: {latest_message}\")\n",
    "\n",
    "    if latest_message.type == \"human\":\n",
    "        latest_message = model_with_tools.invoke(messages)\n",
    "        messages.append(latest_message)\n",
    "    elif latest_message.type == \"ai\":\n",
    "        tool_calls = latest_message.tool_calls\n",
    "        for tool_call in tool_calls:\n",
    "            if tool_call['name'] == \"get_conversion_factor\":\n",
    "                logger.debug(f\"tool_call: {tool_call}\")\n",
    "                tool_result = get_conversion_factor.invoke(tool_call)\n",
    "                logger.debug(f\"get_conversion_factor_result: {tool_result}\")\n",
    "                conversion_factor = tool_result.content\n",
    "                messages.append(tool_result)\n",
    "            elif tool_call['name'] == \"convert_and_display\":\n",
    "                logger.debug(f\"tool_call: {tool_call}\")\n",
    "                tool_call['args']['conversion_factor'] = conversion_factor\n",
    "                tool_result = convert_and_display.invoke(tool_call)\n",
    "                messages.append(tool_result)\n",
    "\n",
    "    elif latest_message.type == \"tool\":\n",
    "        latest_message = model_with_tools.invoke(messages)\n",
    "        messages.append(latest_message)\n",
    "\n",
    "    print (messages)\n",
    "    latest_message = messages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8b7ba49d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='can u convert 1000 USD to INR and display the result as a currencybot message', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to use the function get_conversion_factor to get conversion factor between USD and INR. Then use convert_and_display with base_currency_value? Wait convert_and_display expects base_currency_value: number. But we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So convert_and_display likely uses the conversion factor internally? But we need to provide base_currency_value. It might use the conversion factor from earlier? The function likely uses the conversion factor from get_conversion_factor. But we need to call get_conversion_factor first. Then call convert_and_display with base_currency_value=1000? Or maybe we need to pass the converted value? The description: \"Convert a base currency value to a target currency value using the conversion factor and display the result as a custom CurrencyBot message\". So we provide base_currency_value=1000, and the function will internally use the conversion factor? But we need to pass the conversion factor? The function signature only has base_currency_value. So maybe the function uses the conversion factor from a global context? But we can\\'t guarantee. The typical pattern: call get_conversion_factor, then call convert_and_display with base_currency_value=1000. The function will use the conversion factor from the previous call. So we need to call get_conversion_factor first. Then call convert_and_display. Let\\'s do that.', 'tool_calls': [{'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'function': {'arguments': '{\"base_currency\":\"USD\",\"target_currency\":\"INR\"}', 'name': 'get_conversion_factor'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 322, 'prompt_tokens': 212, 'total_tokens': 534, 'completion_time': 0.318411331, 'prompt_time': 0.048352942, 'queue_time': 0.044301578, 'total_time': 0.366764273}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--8912197c-fb48-4726-8114-0f3dc9f60b07-0', tool_calls=[{'name': 'get_conversion_factor', 'args': {'base_currency': 'USD', 'target_currency': 'INR'}, 'id': 'fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 212, 'output_tokens': 322, 'total_tokens': 534}),\n",
       " ToolMessage(content='88.7311', name='get_conversion_factor', tool_call_id='fc_c91c4e5b-5445-4e85-b0bf-5c0d864bd0e7'),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'We have conversion factor 88.7311. Need to convert 1000 USD to INR. 1000 * 88.7311 = 88731.1 INR. Then use convert_and_display function.', 'tool_calls': [{'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'function': {'arguments': '{\"base_currency_value\":88731.1}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 72, 'prompt_tokens': 252, 'total_tokens': 324, 'completion_time': 0.070586793, 'prompt_time': 0.012164905, 'queue_time': 0.042164894, 'total_time': 0.082751698}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_c5a89987dc', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--6bfb1370-9ab8-4f0d-bea3-1331914d375c-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 88731.1, 'conversion_factor': '88.7311'}, 'id': 'fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c', 'type': 'tool_call'}], usage_metadata={'input_tokens': 252, 'output_tokens': 72, 'total_tokens': 324}),\n",
       " ToolMessage(content='Message from currency conversion bot: the amount is 7873208.10721', name='convert_and_display', tool_call_id='fc_960692cc-332b-48ba-b8e3-8a7c1d2ebd4c'),\n",
       " AIMessage(content='', additional_kwargs={'reasoning_content': 'We need to check the logic. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". We called get_conversion_factor with USD to INR. It returned 88.7311. Then we called convert_and_display with base_currency_value: 88731.1. Wait, base_currency_value should be the amount in base currency? The function convert_and_display expects base_currency_value: number. But the example says: convert_and_display(base_currency_value: number). The function likely expects the base currency value to convert. But we passed 88731.1, which is the result of 1000 * 88.7311. That seems wrong. The function likely expects the base currency value (1000) and will internally multiply by conversion factor. But we passed the converted amount. So the function returned 7873208.10721, which is 88731.1 * 88.7311? Let\\'s check: 88731.1 * 88.7311 = 7873208.10721. Yes. So the function double multiplied. So we need to correct: call convert_and_display with base_currency_value: 1000. And maybe pass conversion_factor? The function signature only has base_currency_value. But maybe the function uses the conversion factor internally. So we should call convert_and_display with base_currency_value: 1000. Let\\'s do that.', 'tool_calls': [{'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'function': {'arguments': '{\"base_currency_value\":1000}', 'name': 'convert_and_display'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 321, 'prompt_tokens': 311, 'total_tokens': 632, 'completion_time': 0.315945389, 'prompt_time': 0.016653005, 'queue_time': 0.043721565, 'total_time': 0.332598394}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_3d587a02fb', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--5057b32e-7874-4ac3-8762-46e0394b17d2-0', tool_calls=[{'name': 'convert_and_display', 'args': {'base_currency_value': 1000, 'conversion_factor': '88.7311'}, 'id': 'fc_3ea0ff73-1d90-4143-acd5-2a7edca79710', 'type': 'tool_call'}], usage_metadata={'input_tokens': 311, 'output_tokens': 321, 'total_tokens': 632}),\n",
       " ToolMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', name='convert_and_display', tool_call_id='fc_3ea0ff73-1d90-4143-acd5-2a7edca79710'),\n",
       " AIMessage(content='Message from currency conversion bot: the amount is 88731.09999999999', additional_kwargs={'reasoning_content': 'We need to respond to the user. The user asked: \"can u convert 1000 USD to INR and display the result as a currencybot message\". The assistant already performed the conversion and displayed the result. The last message from the function is \"Message from currency conversion bot: the amount is 88731.09999999999\". That is the correct conversion. We should just output that message. But we need to ensure we don\\'t call the function again. The last function call returned the message. So we should just output that message. The user might want a final answer. So we should output the message.'}, response_metadata={'token_usage': {'completion_tokens': 152, 'prompt_tokens': 369, 'total_tokens': 521, 'completion_time': 0.149464805, 'prompt_time': 0.019251342, 'queue_time': 0.043139948, 'total_time': 0.168716147}, 'model_name': 'openai/gpt-oss-20b', 'system_fingerprint': 'fp_77f8660d1d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--e2f73d5e-27e0-4f9a-b589-458c84c3e5a8-0', usage_metadata={'input_tokens': 369, 'output_tokens': 152, 'total_tokens': 521})]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "24726e91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Message from currency conversion bot: the amount is 88731.09999999999'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_tools.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee71bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
